{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"omniglot_conv_net.ipynb","provenance":[{"file_id":"1dHrGkR9-ox7PFjK95I-P4q0lbFLkpKWs","timestamp":1578156138844},{"file_id":"1ynYhMh6mfVHyJn9voj-qECMMAqjsxv8B","timestamp":1572630696539},{"file_id":"19Ugd6xcClNpUoRyxAwUMTNXzlR9T7Tq5","timestamp":1566483892779},{"file_id":"1lQ-PEDSlF-3xzhYXbTH-Ze1Q9hASbKP3","timestamp":1566407371074}],"collapsed_sections":["4mTAcOcdUd93","Dg26SMGNUthi","LnO97_TKW26s","84dWSulVdkjl","Pt8kdEYGdpBL","8OM-CAjmeCSV"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m0w2omm1UJ9S","colab_type":"text"},"source":["# Fish classification with nnets\n","Using the [fishdb](http://www.fishdb.co.uk/)"]},{"cell_type":"markdown","metadata":{"id":"FhnOva4D5GZQ","colab_type":"text"},"source":["### TODO\n","- data augmentation ?\n","- other optimizers ?\n","- comparer avec et sans\n","  - batch norm\n","  - batch norm au début\n","  - dropout\n","  - regul"]},{"cell_type":"code","metadata":{"id":"Gph_E9QuwQS4","colab_type":"code","outputId":"2d30bb24-a9c8-434f-9e3b-d23fa51aaba1","executionInfo":{"status":"ok","timestamp":1578215432055,"user_tz":-60,"elapsed":27846,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4mTAcOcdUd93","colab_type":"text"},"source":["## Setup phase\n","We install packages, make all imports, configure modules and download dataset"]},{"cell_type":"code","metadata":{"id":"I1246o-3RW3v","colab_type":"code","outputId":"f14ad687-e403-4085-f060-079559c1b1d7","executionInfo":{"status":"ok","timestamp":1578215540030,"user_tz":-60,"elapsed":135793,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":513}},"source":["%%bash\n","pip install -q pyyaml\n","pip install tensorflow==2.0.0-beta1\n","pip install -q tensorflow-gpu==2.0.0-beta1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0-beta1\n","  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n","Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n","  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.17.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n","Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n","  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (42.0.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.16.0)\n","Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DHAExJYpSHIb","colab_type":"code","outputId":"4470e2b9-e2e2-4d13-f151-02dac252d5f1","executionInfo":{"status":"ok","timestamp":1578215557853,"user_tz":-60,"elapsed":153601,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%bash\n","git clone https://github.com/brendenlake/omniglot\n","mkdir datas\n","unzip -q omniglot/python/images_background.zip -d datas\n","unzip -q omniglot/python/images_evaluation.zip -d datas"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'omniglot'...\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dy0U2QJJRYmd","colab_type":"code","outputId":"127a88e3-dd2a-47d2-80c6-58d49fe16654","executionInfo":{"status":"ok","timestamp":1578215561500,"user_tz":-60,"elapsed":157154,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":513}},"source":["%load_ext tensorboard\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pl\n","import pandas as pd\n","import numpy as np\n","import skimage\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import datasets, layers, models\n","from tensorboard import notebook\n","from keras import backend as K\n","from IPython import display\n","\n","import os, datetime, time, math, pathlib, itertools, random\n","\n","keras = tf.keras\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","print(tf.version.VERSION)\n","print(tf.keras.__version__)\n","print(\"GPU Available: \", tf.test.is_gpu_available())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["2.0.0-beta1\n","2.2.4-tf\n","GPU Available:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_af6yejxduUq","colab_type":"text"},"source":["## Constants\n","This part will define how to build, train, and evaluate the model"]},{"cell_type":"code","metadata":{"id":"l16hgYf3UHel","colab_type":"code","cellView":"both","outputId":"e761b176-e2ac-40bd-934a-c34751f95acb","executionInfo":{"status":"ok","timestamp":1578215561501,"user_tz":-60,"elapsed":157068,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@markdown ## Data\n","DIR_DATAS = \"datas/images_evaluation\" #@param {type:\"string\"}\n","LOAD_FROM = \"\"  #@param [\"\", \"save_dir\"] {allow-input: true}\n","CHECKPOINTS_DIR = \"drive/My Drive/ml/weights/omniglot_conv\" #@param {type:\"string\"}\n","checkpoint_dir_name = \"checkpoint_\" + str(int(time.time()))\n","KEEP_TO_TRAIN = 16 #@param {type:\"number\"}\n","\n","\n","#@markdown ## Model configuration\n","MODEL_TYPE = \"conv_3\" #@param [\"same\", \"linear\", \"dense\", \"conv\", \"conv_2\", \"conv_3\", \"depth_model\", \"depthwise_skip\"]\n","IMG_SIDE = 100 #@param {type:\"slider\", min:10, max:300, step:1}\n","IMG_SHAPE = (IMG_SIDE, IMG_SIDE)\n","NB_CLASSES = 1623 #@param {type:\"number\"}\n","\n","#@markdown ## Training configuration\n","NB_EPOCHS = 40 #@param {type:\"number\"}\n","BATCH_SIZE = 32 #@param {type:\"number\"}\n","TRIPLETS_PER_IMAGE = 10 #@param {type:\"number\"}\n","LEARNING_RATE = 0.001 #@param {type:\"number\"}\n","L2_REGUL = 1e-4 #@param {type:\"number\"}\n","\n","checkpoint_dir_name = \"checkpoint_\" + MODEL_TYPE\n","print(\"Saving in {} for this session\".format(checkpoint_dir_name))\n","if LOAD_FROM:\n","  print(\"Loading weights from checkpoint {}\".format(LOAD_FROM))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Saving in checkpoint_conv_3 for this session\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dg26SMGNUthi","colab_type":"text"},"source":["## General code\n","Helper functions"]},{"cell_type":"code","metadata":{"id":"S7BKheGyefQY","colab_type":"code","colab":{}},"source":["def get_checkpoint_path(suffix=\"\"):\n","  os.makedirs(os.path.join(CHECKPOINTS_DIR, checkpoint_dir_name), exist_ok=True)\n","  return os.path.join(\n","    CHECKPOINTS_DIR,\n","    checkpoint_dir_name,\n","    \"weights\" + suffix + \".hdf5\"\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBDI3NQtQ9u","colab_type":"code","colab":{}},"source":["class Timer():\n","  def __init__(self, to_int = True):\n","    self.t = time.time()\n","    self.to_int = to_int\n","  \n","  def get(self, reset=True):\n","    t2 = time.time()\n","    d = t2 - self.t\n","    if self.to_int:\n","      d = int(d)\n","    if reset:\n","      self.t = t2\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uejqNWClgtrE","colab_type":"code","colab":{}},"source":["def show_image(image):\n","\tplt.imshow(image)\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5d2tWuMVHfA","colab_type":"text"},"source":["## Import datas and pre-processing"]},{"cell_type":"code","metadata":{"id":"gLyuKF73v_Hh","colab_type":"code","colab":{}},"source":["def reshape_image(image):\n","  h, w = image.shape[0], image.shape[1]\n","  scale = min(IMG_SHAPE[0]/h, IMG_SHAPE[1]/w)\n","  padH = round((IMG_SHAPE[0] / scale - h) / 2)\n","  padW = round((IMG_SHAPE[1] / scale - w) / 2)\n","\n","  padShape = ((padH, padH), (padW, padW), (0,0))\n","  image = skimage.util.pad(image, padShape, 'constant')\n","\n","  return skimage.transform.resize(image, IMG_SHAPE, mode='symmetric', preserve_range=True)\n","\n","def preprocess_image(tf_image):\n","  tf_image = tf.image.decode_image(tf_image)\n","  image = tf_image.numpy().astype(float).reshape(tf_image.shape) / 255.0\n","  image = reshape_image(image)\n","  image = np.average(image, axis=2)\n","  return image\n","\n","def load_and_preprocess_image(img_path):\n","  return preprocess_image(tf.io.read_file(str(img_path)))\n","\n","def bin2row(cls_id, row_size=NB_CLASSES):\n","  l = np.zeros(row_size)\n","  l[cls_id] = 1\n","  return l\n","\n","def labels_bin2row(labels):\n","  return np.array([bin2row(i) for i in labels])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5Ha_hh5q9Sq","colab_type":"code","colab":{}},"source":["def list_dir_files(path):\n","  return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","def get_sub_folder_cls(img_classes, img_dirs):\n","  cls = [\n","    [(os.path.join(cls_path, subfolder), subfolder[-2:] + \"_\" + label) for subfolder in os.listdir(cls_path)]\n","    for cls_path, label in zip(img_dirs, img_classes)\n","  ]\n","  cls = list(itertools.chain(*cls))\n","  return [c[1] for c in cls], [c[0] for c in cls]\n","\n","def get_datas_paths(dir_path):\n","  img_classes = sorted(list(os.listdir(dir_path)))\n","  img_dirs = [os.path.join(dir_path, cls) for cls in img_classes]\n","  img_classes, img_dirs = get_sub_folder_cls(img_classes, img_dirs)\n","  # TODO : subdirs\n","  img_infos = [\n","      [(os.path.join(cls_path, img_f_name), label) for img_f_name in list_dir_files(cls_path)]\n","      for cls_path, label in zip(img_dirs, list(range(len(img_dirs))))\n","  ]\n","  img_infos = list(itertools.chain(*img_infos))\n","  paths, labels = [[el[i] for el in img_infos] for i in range(2)]\n","  return paths, labels, img_classes # len : nb images | nb images | nb classes\n","\n","def load_datas(dir_path):\n","  paths, labels, cls_names = get_datas_paths(dir_path)\n","  images_datas = [load_and_preprocess_image(img_path) for img_path in paths]\n","  images_datas, labels = np.array(images_datas), np.array(labels)\n","  return images_datas, labels, cls_names\n","\n","def get_subsets_per_cls(datas, labels, subsets=[None]):\n","  nb_cls = max(labels)+1\n","  subsets_datas = [[() for _ in subsets] for _ in range(nb_cls)]\n","  ids_per_cls = [[] for _ in range(nb_cls)]\n","  for i in range(len(datas)):\n","    ids_per_cls[labels[i]].append(i)\n","  \n","  for i_cls in range(nb_cls):\n","    for i_sub, max_datas in enumerate(subsets):\n","      if max_datas == None:\n","        max_datas = len(ids_per_cls[i_cls])\n","      subsets_datas[i_cls][i_sub] = ids_per_cls[i_cls][:max_datas]\n","      ids_per_cls[i_cls] = ids_per_cls[i_cls][max_datas:]\n","  split_datas = [([], []) for _ in subsets]\n","  for i_cls in range(nb_cls):\n","    for i_sub in range(len(subsets)):\n","      for i_img in subsets_datas[i_cls][i_sub]:\n","        split_datas[i_sub][0].append(datas[i_img])\n","        split_datas[i_sub][1].append(labels[i_img])\n","  split_datas = [(np.array(a1), np.array(a2)) for (a1, a2) in split_datas]\n","  return split_datas"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0jvq2J8UItF","colab_type":"text"},"source":["Now, we read all the datas"]},{"cell_type":"code","metadata":{"id":"EXXibU93QljQ","colab_type":"code","outputId":"45b33125-c4ed-4e14-9604-a81248de9ba8","executionInfo":{"status":"ok","timestamp":1578215603983,"user_tz":-60,"elapsed":198997,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["try:\n","  _ = train_images\n","except:\n","  # train_images, train_labels, train_cls_names = load_datas(DIR_DATAS)\n","  # test_images, test_labels, test_cls_names = train_images, train_labels, train_cls_names\n","  # test_images, test_labels, test_cls_names = load_datas(DIR_DATAS, \"test\")\n","  # train_labels_bin, test_labels_bin = labels_bin2row(train_labels), labels_bin2row(test_labels)\n","\n","  all_images, all_labels, train_cls_names = load_datas(DIR_DATAS)\n","  test_cls_names = train_cls_names\n","  (train_images, train_labels), (test_images, test_labels) = get_subsets_per_cls(all_images, all_labels, (KEEP_TO_TRAIN, None))\n","\n","print(all_images.shape, all_labels.shape)\n","print(train_images.shape, train_labels.shape)\n","print(test_images.shape, test_labels.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(13180, 100, 100) (13180,)\n","(10544, 100, 100) (10544,)\n","(2636, 100, 100) (2636,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LnO97_TKW26s","colab_type":"text"},"source":["## Functions to feed datas to the network"]},{"cell_type":"markdown","metadata":{"id":"7Z7tX_tCazT7","colab_type":"text"},"source":["First, helper functions to :\n","- get all images classed by label"]},{"cell_type":"code","metadata":{"id":"G4utCJ0DW6si","colab_type":"code","colab":{}},"source":["def get_ids_per_cls(labels):\n","  ids_per_cls = []\n","  for i in range(len(labels)):\n","    while len(ids_per_cls) <= labels[i]:\n","      ids_per_cls.append([])\n","    ids_per_cls[labels[i]].append(i)\n","  return ids_per_cls\n","\n","def sort_by_distance(l, anchor, only_ids=True):\n","  l2 = [(dist_fct(el, anchor), i) for i, el in enumerate(l)]\n","  l2.sort()\n","  if only_ids:\n","    return [i for d, i in l2]\n","  return [l[i] for d, i in l2]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USG4PW7hbDJ9","colab_type":"text"},"source":["The following function select random triplets to train the NN"]},{"cell_type":"code","metadata":{"id":"eWsnArbWa-px","colab_type":"code","colab":{}},"source":["def get_triplets_random(images, labels, trunk_model):\n","  nb_images = len(images)\n","  ids_per_cls = get_ids_per_cls(labels)\n","  triplets = []\n","  \n","  for i_anchor in range(nb_images):\n","    same_cls = [i for i in ids_per_cls[labels[i_anchor]] if i != i_anchor]\n","    for _ in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = random.choice(same_cls), i_anchor\n","      while labels[i_negative] == labels[i_anchor]:\n","        i_negative = random.randint(0, nb_images-1)\n","      triplets.append([i_anchor, i_positive, i_negative])\n","  \n","  return triplets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qq90WWVLDrJ","colab_type":"text"},"source":["This functions try to select triplets better than random ones. At first, we run the NN on all examples. Then, we try to select triplets with a positive far from the anchor, and a negative close to it."]},{"cell_type":"code","metadata":{"id":"xtaHxTLzLCzT","colab_type":"code","colab":{}},"source":["# %%time\n","NB_CENTERS_PER_IMAGE = 3\n","# FACT_RANDOM = 3\n","# FACT_RANDOM_POSITIVE = 2\n","\n","def get_triplets_dists(images, labels, trunk_model):\n","  timer = Timer()\n","  print(np.array(images, dtype=np.float32).shape)\n","  coords = trunk_model.predict(np.array(images))\n","  ids_per_cls = get_ids_per_cls(labels)\n","  centers = [np.mean([coords[i] for i in ids_per_cls[lab]]) for lab in range(len(ids_per_cls))]\n","\n","  centers_away_from_cls = [[] for _ in range(len(ids_per_cls))]\n","  for i_cls in range(len(ids_per_cls)):\n","    centers_sorted = sort_by_distance(centers, centers[i_cls])\n","    centers_sorted = [i_center for i_center in centers_sorted if i_center != i_cls][:NB_CENTERS_PER_IMAGE]\n","    centers_away_from_cls[i_cls] = centers_sorted\n","  \n","  triplets = []\n","  useful, unuseful = 0, 0\n","  for anchor in range(len(images)):\n","    same_cls = [i for i in ids_per_cls[labels[anchor]] if i != anchor]\n","    # positives = [random.choice(same_cls) for _ in range(TRIPLETS_PER_IMAGE)]\n","    positives_order = sort_by_distance([coords[i] for i in same_cls], anchor)[::-1]\n","    positives = [same_cls[i] for i in positives_order]\n","    # random.shuffle(positives)\n","\n","    centers_taken = centers_away_from_cls[labels[anchor]]\n","    negatives = list(itertools.chain(*[ids_per_cls[i_cls] for i_cls in centers_taken]))\n","    negatives_order = sort_by_distance([coords[i] for i in negatives], anchor)\n","    negatives = [negatives[i] for i in negatives_order]\n","    # negatives = negatives[:FACT_RANDOM*TRIPLETS_PER_IMAGE]\n","    # random.shuffle(negatives)\n","\n","    for i in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = i%len(positives), i%len(negatives)\n","      dist_diff = dist_fct(coords[anchor], coords[positives[i_positive]]) - dist_fct(coords[anchor], coords[negatives[i_negative]])\n","      if dist_diff + MARGIN < 0:\n","        unuseful += 1\n","      else:\n","        useful += 1\n","        triplets.append([anchor, positives[i_positive], negatives[i_negative]])\n","      # print(triplets[-1], [labels[j] for j in triplets[-1]])\n","  \n","  print(\"\\ntriplets computed\", timer.get(), \"s\", \"(useful, unuseful) =\", (useful, unuseful), \"({:.2f}%)\".format(useful / (useful + unuseful) * 100))\n","\n","  return triplets\n","\n","# triplets = get_triplets_dists(train_images, train_labels, trunk_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw5VslCJbIRI","colab_type":"text"},"source":["Now, we need a function to generate triplets during the training process. This function will be called by `fit_generator`"]},{"cell_type":"code","metadata":{"id":"zIHip7tWbCFy","colab_type":"code","colab":{}},"source":["def create_triplet_generator(images, labels, trunk_model, triplets_getter, batch_size):\n","  triplets = []\n","  cur_triplet = 0\n","  while True:\n","    if cur_triplet + batch_size > len(triplets):\n","      triplets = triplets_getter(images, labels, trunk_model)\n","      random.shuffle(triplets)\n","      cur_triplet = 0\n","    \n","    yield (\n","      [ np.array([images[triplets[cur_triplet + i_triplet][i_in]] for i_triplet in range(batch_size)]) for i_in in range(3)],\n","      [0] * batch_size\n","    )\n","\n","    cur_triplet += batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vx1Ob-kU4JD","colab_type":"text"},"source":["## Building model"]},{"cell_type":"markdown","metadata":{"id":"cu823otOdgkp","colab_type":"text"},"source":["### Base models"]},{"cell_type":"code","metadata":{"id":"enIVJB3exhWY","colab_type":"code","colab":{}},"source":["def create_same_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Flatten(),\n","    # layers.BatchNormalization(),\n","  ], name=\"same_model\")\n","  return model\n","\n","def create_linear_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Flatten(),\n","    layers.Dense(NB_CLASSES, activation='softmax'),\n","  ], name=\"linear_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02rcAoUjUziy","colab_type":"text"},"source":["Model with only fully-connected layers"]},{"cell_type":"code","metadata":{"id":"dX6A30xTUkrf","colab_type":"code","colab":{}},"source":["def create_dense_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(),\n","    layers.Flatten(),\n","    layers.Dense(2048, activation='tanh'),\n","    layers.Dense(1024, activation='tanh'),\n","    # layers.Dense(512, activation='relu'),\n","    layers.Dense(NB_CLASSES, activation='softmax'),\n","  ], name=\"dense_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KDwyNsBAE5R","colab_type":"code","colab":{}},"source":["def get_regul():\n","  return keras.regularizers.l2(L2_REGUL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeQMkXBnthF-","colab_type":"text"},"source":["Convolutional neural networks"]},{"cell_type":"code","metadata":{"id":"hoMNl6P8tktt","colab_type":"code","colab":{}},"source":["def create_conv_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","    layers.Reshape(IMG_SHAPE+(1,)),\n","\n","    layers.Conv2D(20, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(40, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(80, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(4096, activation='tanh'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(2048, activation='tanh'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(NB_CLASSES, activation='softmax')\n","  ], name=\"conv_model\")\n","  return model\n","\n","def create_conv_2_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","    layers.Reshape(IMG_SHAPE+(1,)),\n","\n","    layers.Conv2D(32, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(64, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Flatten(),\n","    layers.Dense(1024, activation='tanh'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(512, activation='tanh'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(NB_CLASSES, activation='softmax')\n","  ], name=\"conv_2_model\")\n","  return model\n","\n","def create_conv_3_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","    layers.Reshape(IMG_SHAPE+(1,)),\n","    \n","    layers.Conv2D(32, (8, 8), activation='relu', padding=\"same\", kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(64, (4, 4), activation='relu', padding=\"same\", kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu', padding=\"same\", kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu', padding=\"same\", kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    # layers.Dense(4096, activation='tanh'),\n","    layers.Dense(NB_CLASSES, activation='softmax')\n","  ], name=\"conv_3_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84dWSulVdkjl","colab_type":"text"},"source":["### Complex blocks"]},{"cell_type":"code","metadata":{"id":"cH4sBtK7dqrv","colab_type":"code","colab":{}},"source":["def create_depthwise_block(id_block, filters, input_tensor):\n","  filters_middle, filters_out = filters\n","  block_name = \"depthwise_{}_\".format(id_block)\n","\n","  x = layers.Conv2D(filters_middle, (1, 1), activation='relu', name=block_name+\"conv_1_extend\")(input_tensor)\n","  x = layers.BatchNormalization(name=block_name+\"batch_norm\")(x)\n","  x = layers.DepthwiseConv2D((3, 3), padding='same', activation='relu', name=block_name+\"depthwise\")(x)\n","  x = layers.Conv2D(filters_out, (1, 1), activation='relu', name=block_name+\"conv_1_regroup\")(x)\n","\n","  return x\n","\n","def create_depthwise_skip_block(id_block, filters, input_tensor):\n","  filters_middle, filters_out = filters\n","  block_name = \"dpw_skip_{}_\".format(id_block)\n","\n","  x = layers.Conv2D(filters_middle, (1, 1), activation='relu', name=block_name+\"conv_1_extend\")(input_tensor)\n","  x = layers.BatchNormalization(name=block_name+\"batch_norm_1\")(x)\n","  x = layers.DepthwiseConv2D((3, 3), padding='same', activation='relu', name=block_name+\"depthwise\")(x)\n","  x = layers.Conv2D(filters_out, (1, 1), activation='relu', name=block_name+\"conv_1_regroup\")(x)\n","\n","  x2 = layers.Conv2D(filters_out, (1, 1), activation='relu', name=block_name+\"skip_connection\")(input_tensor)\n","  x = layers.Add(name=block_name+\"add_outputs\")([x, x2])\n","  x = layers.BatchNormalization(name=block_name+\"batch_norm_2\")(x)\n","\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1A3IpIRnbZ5","colab_type":"code","colab":{}},"source":["def create_classifier_head(input_tensor):\n","  x = layers.Flatten(name=\"head_flatten\")(input_tensor)\n","  x = layers.Dense(1024, activation='tanh', name=\"head_dense_1\")(x)\n","  x = layers.Dropout(0.2, name=\"head_dropout_1\")(x)\n","  x = layers.Dense(512, activation='tanh', name=\"head_dense_2\")(x)\n","  x = layers.Dropout(0.2, name=\"head_dropout_2\")(x)\n","  x = layers.Dense(NB_CLASSES, activation='softmax', name=\"head_softmax_layer\")(x)\n","\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pt8kdEYGdpBL","colab_type":"text"},"source":["### Complex models"]},{"cell_type":"code","metadata":{"id":"YIB6_YzMiCoK","colab_type":"code","colab":{}},"source":["def create_depth_model_model():\n","  input_tensor = layers.Input(shape=IMG_SHAPE)\n","\n","  x = layers.BatchNormalization()(input_tensor)\n","  x = layers.Conv2D(32, (6, 6), activation='relu', padding='same', name=\"dw_first_conv\")(x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_block(1, [64, 32], x)\n","  x = create_depthwise_block(2, [64, 32], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_block(3, [128, 32], x)\n","  x = create_depthwise_block(4, [128, 64], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_block(5, [256, 64], x)\n","  x = create_depthwise_block(6, [256, 128], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_block(7, [256, 64], x)\n","  x = create_depthwise_block(8, [256, 128], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_classifier_head(x)\n","  \n","  return keras.models.Model(input_tensor, x, name='depthwise_model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WelzgAm35FH","colab_type":"code","colab":{}},"source":["def create_depthwise_skip_model():\n","  input_tensor = layers.Input(shape=IMG_SHAPE)\n","\n","  x = layers.BatchNormalization()(input_tensor)\n","  x = layers.Conv2D(32, (6, 6), activation='relu', padding='same', name=\"dw_first_conv\")(x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_skip_block(1, [64, 32], x)\n","  x = create_depthwise_skip_block(2, [64, 32], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_skip_block(3, [128, 32], x)\n","  x = create_depthwise_skip_block(4, [128, 64], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_skip_block(5, [256, 64], x)\n","  x = create_depthwise_skip_block(6, [256, 128], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_depthwise_skip_block(7, [256, 64], x)\n","  x = create_depthwise_skip_block(8, [256, 128], x)\n","  x = layers.MaxPool2D((2, 2))(x)\n","\n","  x = create_classifier_head(x)\n","  \n","  return keras.models.Model(input_tensor, x, name='depthwise_model')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO2W3mjSU_t3","colab_type":"text"},"source":["### Building model"]},{"cell_type":"code","metadata":{"id":"1sp3PzQwUvMw","colab_type":"code","outputId":"f74150a8-3927-4dbb-a7b2-5af1b916e266","executionInfo":{"status":"ok","timestamp":1578215605838,"user_tz":-60,"elapsed":200555,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"source":["model = globals()[\"create_{}_model\".format(MODEL_TYPE)]()\n","model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Model: \"conv_3_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","batch_normalization (BatchNo (None, 100, 100)          400       \n","_________________________________________________________________\n","reshape (Reshape)            (None, 100, 100, 1)       0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 100, 100, 32)      2080      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 50, 50, 32)        0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 50, 50, 32)        128       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 50, 50, 64)        32832     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 25, 25, 64)        0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 25, 25, 64)        256       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 25, 25, 128)       131200    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 12, 12, 256)       524544    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","flatten (Flatten)            (None, 9216)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1623)              14959191  \n","=================================================================\n","Total params: 15,652,167\n","Trainable params: 15,651,007\n","Non-trainable params: 1,160\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pjsdV_yIc2Hr","colab_type":"text"},"source":["If there's a model to restore, we will try to restore weights"]},{"cell_type":"code","metadata":{"id":"zOzNhmkjc1Pw","colab_type":"code","outputId":"b59ef701-1a73-48a0-fd60-51b24b7c7564","executionInfo":{"status":"ok","timestamp":1578215605957,"user_tz":-60,"elapsed":200648,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if LOAD_FROM:\n","  if LOAD_FROM == \"save_dir\":\n","    LOAD_FROM = get_checkpoint_path()\n","  print(\"Load weights from\", LOAD_FROM)\n","  # model.load_weights(LOAD_FROM)\n","  # model = tf.keras.models.load_model(LOAD_FROM)\n","  trunk_model.load_weights(LOAD_FROM)\n","else:\n","  print(\"No weights to load\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["No weights to load\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Hp92KN7U6Mv","colab_type":"text"},"source":["## Training phase"]},{"cell_type":"code","metadata":{"id":"nNxs015TxoRJ","colab_type":"code","colab":{}},"source":["model.compile(\n","    # optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n","    # optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.0001),\n","    # optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.0000001),\n","    loss='sparse_categorical_crossentropy',\n","    # loss='mse',\n","    metrics=['accuracy']\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"34mW85aOyD4B","colab_type":"code","colab":{}},"source":["# !rm -R logs/*\n","# %tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGNJ0OZextKo","colab_type":"code","outputId":"75b5cf03-3f70-4f86-957c-bdc226dfa001","executionInfo":{"status":"error","timestamp":1573561952890,"user_tz":-60,"elapsed":1269779,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","checkpoint_path = get_checkpoint_path(suffix=\"_1.1\")\n","print(\"Saving weights at\", checkpoint_path)\n","\n","callbacks = [\n","  keras.callbacks.ModelCheckpoint(checkpoint_path, load_weights_on_restart=False),\n","  # keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n","]\n","\n","hist = model.fit(\n","  train_images, train_labels,\n","  epochs = 60,\n","  batch_size=BATCH_SIZE,\n","  callbacks=callbacks,\n","  validation_data=(test_images, test_labels)\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saving weights at drive/My Drive/ml/weights/omniglot_conv/checkpoint_conv_3/weights_1.1.hdf5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Train on 10544 samples, validate on 2636 samples\n","Epoch 1/60\n","10544/10544 [==============================] - 19s 2ms/sample - loss: 6.8834 - accuracy: 0.0088 - val_loss: 6.6493 - val_accuracy: 0.0076\n","Epoch 2/60\n","10544/10544 [==============================] - 9s 863us/sample - loss: 5.4096 - accuracy: 0.0746 - val_loss: 5.2487 - val_accuracy: 0.0903\n","Epoch 3/60\n","10544/10544 [==============================] - 9s 894us/sample - loss: 4.5537 - accuracy: 0.1941 - val_loss: 4.6573 - val_accuracy: 0.1745\n","Epoch 4/60\n","10544/10544 [==============================] - 9s 879us/sample - loss: 3.8586 - accuracy: 0.3227 - val_loss: 4.1930 - val_accuracy: 0.2405\n","Epoch 5/60\n","10544/10544 [==============================] - 9s 900us/sample - loss: 3.2440 - accuracy: 0.4571 - val_loss: 3.7807 - val_accuracy: 0.3058\n","Epoch 6/60\n","10544/10544 [==============================] - 9s 880us/sample - loss: 2.7145 - accuracy: 0.5697 - val_loss: 3.4275 - val_accuracy: 0.3581\n","Epoch 7/60\n","10544/10544 [==============================] - 9s 890us/sample - loss: 2.2516 - accuracy: 0.6661 - val_loss: 3.1120 - val_accuracy: 0.4055\n","Epoch 8/60\n","10544/10544 [==============================] - 9s 870us/sample - loss: 1.8588 - accuracy: 0.7409 - val_loss: 2.8561 - val_accuracy: 0.4435\n","Epoch 9/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 1.5425 - accuracy: 0.7969 - val_loss: 2.6474 - val_accuracy: 0.4734\n","Epoch 10/60\n","10544/10544 [==============================] - 9s 863us/sample - loss: 1.2790 - accuracy: 0.8484 - val_loss: 2.4754 - val_accuracy: 0.5027\n","Epoch 11/60\n","10544/10544 [==============================] - 9s 875us/sample - loss: 1.0703 - accuracy: 0.8836 - val_loss: 2.3309 - val_accuracy: 0.5254\n","Epoch 12/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 0.8952 - accuracy: 0.9118 - val_loss: 2.2202 - val_accuracy: 0.5440\n","Epoch 13/60\n","10544/10544 [==============================] - 9s 877us/sample - loss: 0.7483 - accuracy: 0.9340 - val_loss: 2.1186 - val_accuracy: 0.5664\n","Epoch 14/60\n","10544/10544 [==============================] - 9s 893us/sample - loss: 0.6299 - accuracy: 0.9529 - val_loss: 2.0372 - val_accuracy: 0.5763\n","Epoch 15/60\n","10544/10544 [==============================] - 9s 873us/sample - loss: 0.5302 - accuracy: 0.9663 - val_loss: 1.9674 - val_accuracy: 0.5873\n","Epoch 16/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 0.4437 - accuracy: 0.9766 - val_loss: 1.9089 - val_accuracy: 0.5971\n","Epoch 17/60\n","10544/10544 [==============================] - 9s 883us/sample - loss: 0.3752 - accuracy: 0.9842 - val_loss: 1.8550 - val_accuracy: 0.5971\n","Epoch 18/60\n","10544/10544 [==============================] - 9s 899us/sample - loss: 0.3137 - accuracy: 0.9894 - val_loss: 1.8155 - val_accuracy: 0.6039\n","Epoch 19/60\n","10544/10544 [==============================] - 9s 882us/sample - loss: 0.2654 - accuracy: 0.9934 - val_loss: 1.7800 - val_accuracy: 0.6047\n","Epoch 20/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 0.2268 - accuracy: 0.9950 - val_loss: 1.7470 - val_accuracy: 0.6161\n","Epoch 21/60\n","10544/10544 [==============================] - 9s 886us/sample - loss: 0.1947 - accuracy: 0.9966 - val_loss: 1.7206 - val_accuracy: 0.6153\n","Epoch 22/60\n","10544/10544 [==============================] - 9s 894us/sample - loss: 0.1651 - accuracy: 0.9975 - val_loss: 1.6947 - val_accuracy: 0.6210\n","Epoch 23/60\n","10544/10544 [==============================] - 9s 881us/sample - loss: 0.1445 - accuracy: 0.9985 - val_loss: 1.6722 - val_accuracy: 0.6252\n","Epoch 24/60\n","10544/10544 [==============================] - 9s 875us/sample - loss: 0.1267 - accuracy: 0.9988 - val_loss: 1.6426 - val_accuracy: 0.6290\n","Epoch 25/60\n","10544/10544 [==============================] - 10s 908us/sample - loss: 0.1106 - accuracy: 0.9991 - val_loss: 1.6263 - val_accuracy: 0.6282\n","Epoch 26/60\n","10544/10544 [==============================] - 9s 880us/sample - loss: 0.0994 - accuracy: 0.9993 - val_loss: 1.6059 - val_accuracy: 0.6275\n","Epoch 27/60\n","10544/10544 [==============================] - 9s 878us/sample - loss: 0.0885 - accuracy: 0.9996 - val_loss: 1.5924 - val_accuracy: 0.6347\n","Epoch 28/60\n","10544/10544 [==============================] - 9s 900us/sample - loss: 0.0799 - accuracy: 0.9996 - val_loss: 1.5716 - val_accuracy: 0.6396\n","Epoch 29/60\n","10544/10544 [==============================] - 9s 875us/sample - loss: 0.0733 - accuracy: 0.9998 - val_loss: 1.5588 - val_accuracy: 0.6388\n","Epoch 30/60\n","10544/10544 [==============================] - 10s 918us/sample - loss: 0.0668 - accuracy: 0.9997 - val_loss: 1.5464 - val_accuracy: 0.6468\n","Epoch 31/60\n","10544/10544 [==============================] - 9s 893us/sample - loss: 0.0618 - accuracy: 0.9999 - val_loss: 1.5332 - val_accuracy: 0.6468\n","Epoch 32/60\n","10544/10544 [==============================] - 9s 894us/sample - loss: 0.0576 - accuracy: 0.9999 - val_loss: 1.5159 - val_accuracy: 0.6514\n","Epoch 33/60\n","10544/10544 [==============================] - 9s 895us/sample - loss: 0.0540 - accuracy: 0.9996 - val_loss: 1.5056 - val_accuracy: 0.6514\n","Epoch 34/60\n","10544/10544 [==============================] - 9s 876us/sample - loss: 0.0504 - accuracy: 1.0000 - val_loss: 1.4942 - val_accuracy: 0.6517\n","Epoch 35/60\n","10544/10544 [==============================] - 9s 899us/sample - loss: 0.0482 - accuracy: 0.9999 - val_loss: 1.4808 - val_accuracy: 0.6552\n","Epoch 36/60\n","10544/10544 [==============================] - 9s 874us/sample - loss: 0.0454 - accuracy: 0.9999 - val_loss: 1.4683 - val_accuracy: 0.6533\n","Epoch 37/60\n","10544/10544 [==============================] - 10s 905us/sample - loss: 0.0435 - accuracy: 0.9999 - val_loss: 1.4632 - val_accuracy: 0.6525\n","Epoch 38/60\n","10544/10544 [==============================] - 9s 882us/sample - loss: 0.0417 - accuracy: 1.0000 - val_loss: 1.4569 - val_accuracy: 0.6571\n","Epoch 39/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 0.0402 - accuracy: 1.0000 - val_loss: 1.4437 - val_accuracy: 0.6627\n","Epoch 40/60\n","10544/10544 [==============================] - 9s 869us/sample - loss: 0.0387 - accuracy: 1.0000 - val_loss: 1.4374 - val_accuracy: 0.6616\n","Epoch 41/60\n","10544/10544 [==============================] - 9s 894us/sample - loss: 0.0376 - accuracy: 1.0000 - val_loss: 1.4285 - val_accuracy: 0.6635\n","Epoch 42/60\n","10544/10544 [==============================] - 9s 876us/sample - loss: 0.0364 - accuracy: 1.0000 - val_loss: 1.4214 - val_accuracy: 0.6635\n","Epoch 43/60\n","10544/10544 [==============================] - 9s 880us/sample - loss: 0.0357 - accuracy: 1.0000 - val_loss: 1.4143 - val_accuracy: 0.6654\n","Epoch 44/60\n","10544/10544 [==============================] - 9s 895us/sample - loss: 0.0348 - accuracy: 1.0000 - val_loss: 1.4127 - val_accuracy: 0.6677\n","Epoch 45/60\n","10544/10544 [==============================] - 9s 869us/sample - loss: 0.0340 - accuracy: 1.0000 - val_loss: 1.4037 - val_accuracy: 0.6703\n","Epoch 46/60\n","10544/10544 [==============================] - 9s 900us/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 1.4014 - val_accuracy: 0.6669\n","Epoch 47/60\n","10544/10544 [==============================] - 9s 879us/sample - loss: 0.0329 - accuracy: 1.0000 - val_loss: 1.3936 - val_accuracy: 0.6692\n","Epoch 48/60\n","10544/10544 [==============================] - 9s 896us/sample - loss: 0.0323 - accuracy: 1.0000 - val_loss: 1.3909 - val_accuracy: 0.6688\n","Epoch 49/60\n","10544/10544 [==============================] - 9s 874us/sample - loss: 0.0319 - accuracy: 1.0000 - val_loss: 1.3839 - val_accuracy: 0.6745\n","Epoch 50/60\n","10544/10544 [==============================] - 9s 890us/sample - loss: 0.0313 - accuracy: 1.0000 - val_loss: 1.3826 - val_accuracy: 0.6756\n","Epoch 51/60\n","10544/10544 [==============================] - 9s 873us/sample - loss: 0.0309 - accuracy: 1.0000 - val_loss: 1.3824 - val_accuracy: 0.6749\n","Epoch 52/60\n","10544/10544 [==============================] - 9s 897us/sample - loss: 0.0305 - accuracy: 1.0000 - val_loss: 1.3745 - val_accuracy: 0.6745\n","Epoch 53/60\n","10544/10544 [==============================] - 9s 881us/sample - loss: 0.0301 - accuracy: 1.0000 - val_loss: 1.3739 - val_accuracy: 0.6810\n","Epoch 54/60\n"," 6464/10544 [=================>............] - ETA: 3s - loss: 0.0297 - accuracy: 1.0000"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8OM-CAjmeCSV","colab_type":"text"},"source":["## Functions to compute / plot stats about trained models"]},{"cell_type":"code","metadata":{"id":"3HYfUdZVeBs8","colab_type":"code","colab":{}},"source":["def eval_accuracy(model, images, labels):\n","  success = [False for _ in labels]\n","  predictions = model.predict(images, BATCH_SIZE)\n","  predictions = [p.argmax() for p in predictions]\n","  success = [a==b for a, b in zip(labels, predictions)]\n","  acc = sum(success) / len(images)\n","  return acc, success, predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Nm2QVlpHmdF","colab_type":"code","colab":{}},"source":["def plot_history_key(histories, key='loss', color=None):\n","  plt.figure(figsize=(14,8))\n","\n","  for name, history in histories:\n","    val = plt.plot(history.epoch, history.history['val_'+key],\n","                   '--', label=name.title()+' validation '+key, color=color)\n","    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n","             label=name.title()+' train '+key)\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(key.replace('_',' ').title())\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])\n","\n","def plot_history(history, name=\"\", accuracy_key=\"accuracy\"):\n","  history = [(name, history)]\n","  plot_history_key(history, \"loss\", color=\"blue\")\n","  plot_history_key(history, accuracy_key, color=\"green\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brEHu0gle5Dg","colab_type":"code","colab":{}},"source":["def plot_hist(arrs):\n","  plt.figure(figsize=(12,5))\n","  plt.hist(arrs,\n","    bins = 60,\n","    color = ['blue', '#D72F1A'],\n","    # edgecolor = 'black',\n","    label=[\"Same dists\", \"Diff dists\"],\n","    density=True\n","  )\n","  plt.legend(loc='upper right')\n","\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-m-84ILPFYx","colab_type":"code","colab":{}},"source":["def plot_results_per_class(is_success, labels, cls_names):\n","  n_cls = len(cls_names)\n","  cls_success, cls_failed = [0]*n_cls, [0]*n_cls\n","\n","  plt.figure(figsize=(14.5,6))\n","\n","  for i, succ in enumerate(is_success):\n","    if succ == True:\n","      cls_success[labels[i]] += 1\n","    elif succ == False:\n","      cls_failed[labels[i]] += 1\n","\n","  # cls_sum = [max(a+b, 1) for a, b in zip(cls_success, cls_failed)]\n","  # cls_success = [v / s for v, s in zip(cls_success, cls_sum)]\n","  # cls_failed = [v / s for v, s in zip(cls_failed, cls_sum)]\n","  \n","  ind = np.arange(n_cls)\n","  width = 0.8 # the width of the bars: can also be len(x) sequence\n","  rotation = 45 if n_cls < 60 else 90\n","\n","  p1 = plt.bar(ind, cls_success, width, color=\"#4CAF50\")\n","  p2 = plt.bar(ind, cls_failed, width, bottom=cls_success, color=\"#EF5350\")\n","\n","  plt.ylabel('Number of tests')\n","  plt.xlabel('Fish species')\n","  plt.title('Number of detection success and failure per fish species')\n","  plt.xticks(ind, cls_names, rotation=rotation)\n","  # plt.yticks(np.arange(0, 81, 10))\n","  plt.legend((p1[0], p2[0]), ('Success', 'Failed'))\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw6KfMEPU7-h","colab_type":"text"},"source":["## Display trained model stats"]},{"cell_type":"code","metadata":{"id":"ycIwPJ1i-MkT","colab_type":"code","colab":{}},"source":["# plot_history(hist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_TylF5AWw56","colab_type":"code","colab":{}},"source":["print(\"===== TRAINING STATS =====\")\n","accuracy, good_results, predictions = eval_accuracy(model, train_images, train_labels)\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","plot_results_per_class(good_results, train_labels, train_cls_names)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKOpRm8wdIdr","colab_type":"code","colab":{}},"source":["print(\"===== TESTING STATS =====\")\n","accuracy, good_results, predictions = eval_accuracy(model, test_images, test_labels)\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","plot_results_per_class(good_results, test_labels, test_cls_names)"],"execution_count":0,"outputs":[]}]}