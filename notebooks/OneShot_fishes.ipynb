{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OneShot_fishes.ipynb","provenance":[{"file_id":"1lQ-PEDSlF-3xzhYXbTH-Ze1Q9hASbKP3","timestamp":1566407371074}],"collapsed_sections":["4mTAcOcdUd93","Dg26SMGNUthi","p5d2tWuMVHfA","5vx1Ob-kU4JD","jgTfHHLOHrmC","wtCQ8tilIKkk","nvrjm90Ficov","RofKjrjsqobE"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m0w2omm1UJ9S","colab_type":"text"},"source":["# One-shot learning implementation for fish recognition\n","Using the [fish4knowledge Fish Recognition Ground-Truth dataset](https://github.com/brendenlake/omniglot/tree/master/python)"]},{"cell_type":"markdown","metadata":{"id":"FhnOva4D5GZQ","colab_type":"text"},"source":["### TODO\n","- entrainer sans negatif ?\n","- normalization des entrées (pour same ?)\n","- data augmentation ?\n","- comparer avec et sans\n","  - batch norm\n","  - batch norm au début\n","  - dropout\n","  - regul"]},{"cell_type":"code","metadata":{"id":"Gph_E9QuwQS4","colab_type":"code","outputId":"61b6b2dd-6cbb-446f-c6e3-e24e4f31c5cc","executionInfo":{"status":"ok","timestamp":1584542652232,"user_tz":-60,"elapsed":2849,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4mTAcOcdUd93","colab_type":"text"},"source":["## Setup phase\n","We install packages, make all imports, configure modules and download dataset"]},{"cell_type":"code","metadata":{"id":"I1246o-3RW3v","colab_type":"code","outputId":"d9944cbc-1b22-486c-82dd-15e9fa178682","executionInfo":{"status":"ok","timestamp":1584542699863,"user_tz":-60,"elapsed":50439,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%%bash\n","pip install -q pyyaml\n","pip install tensorflow==2.0.0-beta1\n","pip install -q tensorflow-gpu==2.0.0-beta1"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0-beta1\n","  Using cached https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.18.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n","Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.34.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.24.3)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (45.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n","Installing collected packages: tensorflow\n","  Found existing installation: tensorflow 2.1.0\n","    Uninstalling tensorflow-2.1.0:\n","      Successfully uninstalled tensorflow-2.1.0\n","Successfully installed tensorflow-2.0.0b1\n"],"name":"stdout"},{"output_type":"stream","text":["ERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 2.0.0b1 which is incompatible.\n","ERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"DHAExJYpSHIb","colab_type":"code","outputId":"3babd7e5-d7fa-4c32-e9f4-a67de2e9db01","executionInfo":{"status":"ok","timestamp":1584542699866,"user_tz":-60,"elapsed":50356,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%%bash\n","mkdir datas\n","unzip -q \"/content/drive/My Drive/ml/datas/fishes_species.zip\" -d datas\n","unzip -q \"/content/drive/My Drive/ml/datas/fishes_one_shot.zip\" -d datas"],"execution_count":18,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘datas’: File exists\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [unzip -q ]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [\"/content]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [/drive/My]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ Drive/ml]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [/datas/fi]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [shes_one_]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [shot.zip\"]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ -d datas]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [{ENTER}]\n","replace datas/fishes_species/Pleuronectus americanus/8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n","(EOF or read error, treating as \"[N]one\" ...)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dy0U2QJJRYmd","colab_type":"code","outputId":"76d42f63-4aba-4a79-d5b8-d499e2d45983","executionInfo":{"status":"ok","timestamp":1584542699867,"user_tz":-60,"elapsed":50321,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%load_ext tensorboard\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pl\n","import pandas as pd\n","import numpy as np\n","import skimage\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import datasets, layers, models\n","from tensorboard import notebook\n","from keras import backend as K\n","from IPython import display\n","\n","import os, datetime, time, math, pathlib, itertools, random\n","\n","keras = tf.keras\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","print(tf.version.VERSION)\n","print(tf.keras.__version__)\n","print(\"GPU Available: \", tf.test.is_gpu_available())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n","2.0.0-beta1\n","2.2.4-tf\n","GPU Available:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_af6yejxduUq","colab_type":"text"},"source":["## Constants\n","This part will define how to build, train, and evaluate the model"]},{"cell_type":"code","metadata":{"id":"l16hgYf3UHel","colab_type":"code","cellView":"both","outputId":"d6d30168-b21d-4fcb-be04-ae620e41c89f","executionInfo":{"status":"ok","timestamp":1584542699868,"user_tz":-60,"elapsed":50188,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@markdown ## Data paths\n","DIR_TRAIN = \"datas/fishes_species\" #@param {type:\"string\"}\n","DIR_TEST = \"datas/fishes_one_shot\" #@param {type:\"string\"}\n","LOAD_FROM = \"\"  #@param [\"\", \"save_dir\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_pretrained_1/weights_4_22_25.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_conv/weights.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_conv2/weights.hdf5\"] {allow-input: true}\n","CHECKPOINTS_DIR = \"drive/My Drive/ml/weights/oneshot_fishes\"\n","checkpoint_dir_name = \"checkpoint_\" + str(int(time.time()))\n","\n","\n","#@markdown ## Model configuration\n","MODEL_TYPE = \"conv_2\" #@param [\"same\", \"density\", \"linear\",\"dense\", \"conv\", \"conv_2\", \"conv_3\", \"pretrained_1\"]\n","IMG_SIDE = 128 #@param {type:\"slider\", min:10, max:300, step:1}\n","# -> [96, 128, 160, 192, 224]\n","IMG_SHAPE = (IMG_SIDE, IMG_SIDE, 3)\n","\n","#@markdown ## Training configuration\n","NB_EPOCHS = 100 #@param {type:\"number\"}\n","BATCH_SIZE = 32 #@param {type:\"number\"}\n","TRIPLETS_PER_IMAGE = 10 #@param {type:\"number\"}\n","LEARNING_RATE = 0.0000000001 #@param {type:\"number\"}\n","MARGIN = 1 #@param {type:\"number\"}\n","\n","#@markdown ## Evaluation configuration\n","ACCURACY_SAMPLE_SIZE = 200 #@param {type:\"number\"}\n","KNOWN_BASE = 10 #@param {type:\"number\"}\n","TESTING_DATAS = 100 #@param {type:\"number\"}\n","\n","checkpoint_dir_name = \"checkpoint_\" + MODEL_TYPE\n","print(\"Saving in {} for this session\".format(checkpoint_dir_name))\n","if LOAD_FROM:\n","  print(\"Loading weights from checkpoint {}\".format(LOAD_FROM))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Saving in checkpoint_conv_2 for this session\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dg26SMGNUthi","colab_type":"text"},"source":["## General code\n","Helper functions"]},{"cell_type":"code","metadata":{"id":"435-vCZcqPyc","colab_type":"code","colab":{}},"source":["def getRandomIds(dataset, nMax=1000):\n","\tids = list(range(len(dataset[0][0])-1))\n","\trandom.shuffle(ids)\n","\tids = ids[:nMax]\n","\treturn ids + [i+1 for i in ids]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG591ol1s-zz","colab_type":"code","colab":{}},"source":["  def dist_fct(x, y):\n","    return np.sqrt(np.sum((x-y)**2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7BKheGyefQY","colab_type":"code","colab":{}},"source":["def get_checkpoint_path(suffix=\"\"):\n","  os.makedirs(os.path.join(CHECKPOINTS_DIR, checkpoint_dir_name), exist_ok=True)\n","  return os.path.join(\n","    CHECKPOINTS_DIR,\n","    checkpoint_dir_name,\n","    \"weights\" + suffix + \".hdf5\"\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBDI3NQtQ9u","colab_type":"code","colab":{}},"source":["class Timer():\n","  def __init__(self, to_int = True):\n","    self.t = time.time()\n","    self.to_int = to_int\n","  \n","  def get(self, reset=True):\n","    t2 = time.time()\n","    d = t2 - self.t\n","    if self.to_int:\n","      d = int(d)\n","    if reset:\n","      self.t = t2\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuTeqs4BqpS6","colab_type":"code","colab":{}},"source":["def plot_history(histories, key='binary_crossentropy'):\n","  plt.figure(figsize=(16,10))\n","\n","  for name, history in histories:\n","    val = plt.plot(history.epoch, history.history['val_'+key],\n","                   '--', label=name.title()+' Val')\n","    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n","             label=name.title()+' Train')\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(key.replace('_',' ').title())\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uejqNWClgtrE","colab_type":"code","colab":{}},"source":["def show_image(image):\n","\tplt.imshow(image)\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5d2tWuMVHfA","colab_type":"text"},"source":["## Import datas and pre-processing"]},{"cell_type":"code","metadata":{"id":"gLyuKF73v_Hh","colab_type":"code","colab":{}},"source":["def img_to_rgb(image):\n","  if len(image.shape) == 3 and image.shape[2] == 3:\n","    return image\n","  if len(image.shape) == 3:\n","    image = image.reshape(image.shape[:2])\n","  return skimage.color.grey2rgb(image)\n","\n","def reshape_image(image):\n","  h, w = image.shape[0], image.shape[1]\n","  scale = min(IMG_SHAPE[0]/h, IMG_SHAPE[1]/w)\n","  padH = round((IMG_SHAPE[0] / scale - h) / 2)\n","  padW = round((IMG_SHAPE[1] / scale - w) / 2)\n","\n","  padShape = ((padH, padH), (padW, padW), (0,0))\n","  image = skimage.util.pad(image, padShape, 'constant')\n","\n","  return skimage.transform.resize(image, IMG_SHAPE, mode='symmetric', preserve_range=True)\n","\n","def preprocess_image(tf_image):\n","  tf_image = tf.image.decode_image(tf_image)\n","  # tf_image = tf.image.resize(tf_image, IMG_SHAPE)\n","  image = tf_image.numpy().astype(float).reshape(tf_image.shape) / 255.0\n","  image = img_to_rgb(image)\n","  image = reshape_image(image)\n","  return image\n","\n","def load_and_preprocess_image(img_path):\n","  return preprocess_image(tf.io.read_file(str(img_path)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5Ha_hh5q9Sq","colab_type":"code","colab":{}},"source":["def get_datas_paths(dir_path):\n","  fish_classes = list(os.listdir(dir_path))\n","  fish_dirs = [os.path.join(dir_path, cls) for cls in fish_classes]\n","  img_infos = [\n","      [(os.path.join(cls_path, img_f_name), label) for img_f_name in os.listdir(cls_path)]\n","      for cls_path, label in zip(fish_dirs, list(range(len(fish_dirs))))\n","  ]\n","  img_infos = list(itertools.chain(*img_infos))\n","  paths, labels = [[el[i] for el in img_infos] for i in range(2)]\n","  return paths, labels, fish_classes # len : nb images | nb images | nb classes\n","\n","def load_datas(dir_path):\n","  paths, labels, cls_names = get_datas_paths(dir_path)\n","  images_datas = [load_and_preprocess_image(img_path) for img_path in paths]\n","  return images_datas, labels, cls_names\n","\n","def get_subsets_per_cls(datas, labels, subsets=[None]):\n","  nb_cls = max(labels)+1\n","  subsets_datas = [[() for _ in subsets] for _ in range(nb_cls)]\n","  ids_per_cls = [[] for _ in range(nb_cls)]\n","  for i in range(len(datas)):\n","    ids_per_cls[labels[i]].append(i)\n","  \n","  for i_cls in range(nb_cls):\n","    for i_sub, max_datas in enumerate(subsets):\n","      if max_datas == None:\n","        max_datas = len(ids_per_cls[i_cls])\n","      subsets_datas[i_cls][i_sub] = ids_per_cls[i_cls][:max_datas]\n","      ids_per_cls[i_cls] = ids_per_cls[i_cls][max_datas:]\n","  split_datas = [([], []) for _ in subsets]\n","  for i_cls in range(nb_cls):\n","    for i_sub in range(len(subsets)):\n","      for i_img in subsets_datas[i_cls][i_sub]:\n","        split_datas[i_sub][0].append(datas[i_img])\n","        split_datas[i_sub][1].append(labels[i_img])\n","  return split_datas"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0jvq2J8UItF","colab_type":"text"},"source":["Now, we read all the datas"]},{"cell_type":"code","metadata":{"id":"EXXibU93QljQ","colab_type":"code","colab":{}},"source":["from skimage import transform as sktrans\n","\n","try:\n","  _ = train_images\n","except:\n","  print(\"Load datas...\")\n","  train_images, train_labels, train_cls_names = load_datas(DIR_TRAIN)\n","  test_images_load, test_labels_load, test_cls_names = load_datas(DIR_TEST)\n","  (test_base_images, test_base_labels), (test_images, test_labels) = get_subsets_per_cls(test_images_load, test_labels_load, [KNOWN_BASE, TESTING_DATAS])\n","\n","  train_images += test_base_images\n","  train_labels += [i+len(train_cls_names) for i in test_base_labels]\n","  train_cls_names += test_cls_names\n","\n","# train_images, train_labels, train_cls_names = test_base_images, test_base_labels, test_cls_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LnO97_TKW26s","colab_type":"text"},"source":["## Functions to feed datas to the network"]},{"cell_type":"markdown","metadata":{"id":"7Z7tX_tCazT7","colab_type":"text"},"source":["First, helper functions to :\n","- get all images classed by label"]},{"cell_type":"code","metadata":{"id":"G4utCJ0DW6si","colab_type":"code","colab":{}},"source":["def get_ids_per_cls(labels):\n","  ids_per_cls = []\n","  for i in range(len(labels)):\n","    while len(ids_per_cls) <= labels[i]:\n","      ids_per_cls.append([])\n","    ids_per_cls[labels[i]].append(i)\n","  return ids_per_cls\n","\n","def sort_by_distance(l, anchor, only_ids=True):\n","  l2 = [(dist_fct(el, anchor), i) for i, el in enumerate(l)]\n","  l2.sort()\n","  if only_ids:\n","    return [i for d, i in l2]\n","  return [l[i] for d, i in l2]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USG4PW7hbDJ9","colab_type":"text"},"source":["The following function select random triplets to train the NN"]},{"cell_type":"code","metadata":{"id":"eWsnArbWa-px","colab_type":"code","colab":{}},"source":["def get_triplets_random(images, labels, trunk_model):\n","  nb_images = len(images)\n","  ids_per_cls = get_ids_per_cls(labels)\n","  triplets = []\n","  \n","  for i_anchor in range(nb_images):\n","    same_cls = [i for i in ids_per_cls[labels[i_anchor]] if i != i_anchor]\n","    for _ in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = random.choice(same_cls), i_anchor\n","      while labels[i_negative] == labels[i_anchor]:\n","        i_negative = random.randint(0, nb_images-1)\n","      triplets.append([i_anchor, i_positive, i_negative])\n","  \n","  return triplets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qq90WWVLDrJ","colab_type":"text"},"source":["This functions try to select triplets better than random ones. At first, we run the NN on all examples. Then, we try to select triplets with a positive far from the anchor, and a negative close to it."]},{"cell_type":"code","metadata":{"id":"xtaHxTLzLCzT","colab_type":"code","colab":{}},"source":["# %%time\n","NB_CENTERS_PER_IMAGE = 3\n","# FACT_RANDOM = 3\n","# FACT_RANDOM_POSITIVE = 2\n","\n","def get_triplets_dists(images, labels, trunk_model):\n","  timer = Timer()\n","  coords = trunk_model.predict(np.array(images))\n","  ids_per_cls = get_ids_per_cls(labels)\n","  centers = [np.mean([coords[i] for i in ids_per_cls[lab]], axis=0) for lab in range(len(ids_per_cls))]\n","\n","  centers_away_from_cls = [[] for _ in range(len(ids_per_cls))]\n","  for i_cls in range(len(ids_per_cls)):\n","    centers_sorted = sort_by_distance(centers, centers[i_cls])\n","    centers_sorted = [i_center for i_center in centers_sorted if i_center != i_cls][:NB_CENTERS_PER_IMAGE]\n","    centers_away_from_cls[i_cls] = centers_sorted\n","  \n","  triplets = []\n","  useful, unuseful = 0, 0\n","  for anchor in range(len(images)):\n","    same_cls = [i for i in ids_per_cls[labels[anchor]] if i != anchor] or [anchor]\n","    # positives = [random.choice(same_cls) for _ in range(TRIPLETS_PER_IMAGE)]\n","    positives_order = sort_by_distance([coords[i] for i in same_cls], anchor)[::-1]\n","    positives = [same_cls[i] for i in positives_order]\n","    # random.shuffle(positives)\n","\n","    centers_taken = centers_away_from_cls[labels[anchor]]\n","    negatives = list(itertools.chain(*[ids_per_cls[i_cls] for i_cls in centers_taken]))\n","    negatives_order = sort_by_distance([coords[i] for i in negatives], anchor)\n","    negatives = [negatives[i] for i in negatives_order]\n","    # negatives = negatives[:FACT_RANDOM*TRIPLETS_PER_IMAGE]\n","    # random.shuffle(negatives)\n","\n","    for i in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = i%len(positives), i%len(negatives)\n","      dist_diff = dist_fct(coords[anchor], coords[positives[i_positive]]) - dist_fct(coords[anchor], coords[negatives[i_negative]])\n","      if dist_diff + MARGIN < 0:\n","        unuseful += 1\n","      else:\n","        useful += 1\n","        triplets.append([anchor, positives[i_positive], negatives[i_negative]])\n","      # print(triplets[-1], [labels[j] for j in triplets[-1]])\n","  \n","  print(\"\\ntriplets computed\", timer.get(), \"s\", \"(useful, unuseful) =\", (useful, unuseful), \"({:.2f}%)\".format(useful / (useful + unuseful) * 100))\n","\n","  return triplets\n","\n","# triplets = get_triplets_dists(train_images, train_labels, trunk_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw5VslCJbIRI","colab_type":"text"},"source":["Now, we need a function to generate triplets during the training process. This function will be called by `fit_generator`"]},{"cell_type":"code","metadata":{"id":"zIHip7tWbCFy","colab_type":"code","colab":{}},"source":["def create_triplet_generator(images, labels, trunk_model, triplets_getter, batch_size):\n","  triplets = []\n","  cur_triplet = 0\n","  while True:\n","    if cur_triplet + batch_size > len(triplets):\n","      triplets = triplets_getter(images, labels, trunk_model)\n","      random.shuffle(triplets)\n","      cur_triplet = 0\n","    \n","    yield (\n","      [ np.array([images[triplets[cur_triplet + i_triplet][i_in]] for i_triplet in range(batch_size)]) for i_in in range(3)],\n","      [0] * batch_size\n","    )\n","\n","    cur_triplet += batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vx1Ob-kU4JD","colab_type":"text"},"source":["## Model definition"]},{"cell_type":"markdown","metadata":{"id":"ZXnRDBC0HwN0","colab_type":"text"},"source":["### Simple models"]},{"cell_type":"code","metadata":{"id":"ZJpPZYsIH2i2","colab_type":"code","colab":{}},"source":["def create_same_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Flatten(),\n","  ], name=\"same_model\")\n","  return model\n","\n","def create_density_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Lambda(lambda x : tf.math.reduce_mean(x, axis=(1,2)))\n","    # layers.Flatten(),\n","  ], name=\"same_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dX6A30xTUkrf","colab_type":"code","colab":{}},"source":["def create_dense_trunk_model():\n","  model = keras.models.Sequential([\n","    get_mobile_net(train_layers=11), # 11\n","    # layers.GlobalAveragePooling2D(),\n","    layers.Flatten(),\n","    layers.Dense(2000, activation='relu'),\n","    layers.Dense(400, activation='relu'), # 64 ?\n","  ], name=\"dense_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlQsyRUdH9-A","colab_type":"code","colab":{}},"source":["def create_conv_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(20, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(40, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(300, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(100, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model\")\n","  return model\n","\n","def create_conv_2_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(),\n","\n","    layers.Conv2D(64, (8, 8), activation='relu'), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (8, 8), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Flatten(),\n","    layers.Dense(2048, activation='tanh'),\n","    layers.Dense(32 , activation='tanh'),\n","\n","    # layers.Flatten(),\n","    # layers.Dense(300, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(100, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model_2\")\n","  return model\n","\n","def create_conv_3_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(), # TODO ? \n","\n","    layers.Conv2D(64, (8, 8), activation='relu'), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (8, 8), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu'),\n","\n","    layers.Flatten(),\n","    layers.Dense(2048, activation='tanh'), # TODO : 4096 ?\n","    layers.Dense(16, activation='tanh'), # TODO : 64 ?\n","  ], name=\"conv_model_3\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgTfHHLOHrmC","colab_type":"text"},"source":["### With mobile net"]},{"cell_type":"code","metadata":{"id":"16g9L4aUwCEK","colab_type":"code","colab":{}},"source":["def get_mobile_net(train_layers=0):\n","  model = tf.keras.applications.MobileNetV2(\n","    input_shape=IMG_SHAPE,\n","    include_top=False,\n","    weights='imagenet'\n","  )\n","  model.trainable = False\n","  if train_layers:\n","    model.trainable = True\n","    for layer in model.layers[:-train_layers]:\n","      layer.trainable =  False\n","  return model\n","\n","def create_linear_trunk_model():\n","  model = keras.models.Sequential([\n","    get_mobile_net(train_layers=0), # 11\n","    # layers.GlobalAveragePooling2D(),\n","    layers.Flatten(),\n","    layers.Dense(200, activation='relu'),\n","  ], name=\"linear_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtCQ8tilIKkk","colab_type":"text"},"source":["### With pre-trained model"]},{"cell_type":"code","metadata":{"id":"7uWBDd1cIxhA","colab_type":"code","colab":{}},"source":["def get_pretrained_back(remove=1, trainable=2):\n","  checkpoint_path = \"drive/My Drive/ml/weights/fishes/checkpoint_conv_2/weights_1.hdf5\"\n","  # model = keras.models.load_model(checkpoint_path, compile=False)\n","\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE), layers.BatchNormalization(),\n","    layers.Conv2D(32, (5, 5), activation='relu'),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(64, (5, 5), activation='relu'),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(128, (4, 4), activation='relu'),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(256, (4, 4), activation='relu'),layers.MaxPool2D((2, 2)),\n","    layers.Flatten(),layers.Dense(1024, activation='tanh'),layers.Dropout(0.2),\n","    layers.Dense(512, activation='tanh'),layers.Dropout(0.2),layers.Dense(23, activation='softmax')\n","  ], name=\"conv_2_model\")\n","\n","  model.load_weights(checkpoint_path)\n","\n","  for _ in range(remove):\n","    model.pop()\n","  \n","  for l in model.layers[:len(model.layers)-trainable]:\n","    l.trainable = False\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GKR-VSFXJFpk","colab_type":"code","colab":{}},"source":["def create_pretrained_1_trunk_model():\n","  model = get_pretrained_back(remove=4, trainable=4)\n","  model.add(layers.Dense(32, activation=\"sigmoid\"))\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02rcAoUjUziy","colab_type":"text"},"source":["### Create the siamese network"]},{"cell_type":"markdown","metadata":{"id":"RlCDXXUVU_Mp","colab_type":"text"},"source":["The first function, given a model, create a siamese NN with this model as the common part. The second create a model with two times the same siamese network to compute triplet loss."]},{"cell_type":"code","metadata":{"id":"OoW1fHWqU-SL","colab_type":"code","colab":{}},"source":["def create_siamese(trunk_model):\n","  inputs = [layers.Input(IMG_SHAPE) for _ in range(2)]\n","  parts = [trunk_model(inTensor) for inTensor in inputs]\n","  diff = layers.subtract(parts)\n","  out = layers.Lambda(lambda x : tf.reduce_sum(x**2, axis=(1,)))(diff)\n","  out_sqrt = layers.Lambda(lambda x : tf.sqrt(x))(out)\n","  return keras.models.Model(inputs=inputs, outputs=out_sqrt, name=\"Siamese_model\"+\"_\"+trunk_model.name)\n","\n","def create_triplet_siamese(siamese_model, margin=1.0):\n","  in_anchor, in_positive, in_negative = [layers.Input(IMG_SHAPE, name=name) for name in [\"in_anchor\", \"in_positive\", \"in_negative\"]]\n","  positive_dist = siamese_model([in_anchor, in_positive])\n","  negative_dist = siamese_model([in_anchor, in_negative])\n","\n","  dist = layers.subtract([positive_dist, negative_dist])\n","  if margin:\n","    dist = layers.Lambda(lambda x : tf.maximum(x + margin, 0.0))(dist)\n","  # dist = layers.Lambda(lambda x : tf.square(x))(dist) # keep ?\n","  return keras.models.Model(inputs=[in_anchor, in_positive, in_negative], outputs=dist, name=\"Siamese_triplet_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO2W3mjSU_t3","colab_type":"text"},"source":["We will now choose the model we will use"]},{"cell_type":"code","metadata":{"id":"1sp3PzQwUvMw","colab_type":"code","outputId":"dcb27bac-c250-4191-f5f1-76d6a33621a3","executionInfo":{"status":"ok","timestamp":1584542701055,"user_tz":-60,"elapsed":49976,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["trunk_model = globals()[\"create_{}_trunk_model\".format(MODEL_TYPE)]()\n","siamese_model = create_siamese(trunk_model)\n","model = create_triplet_siamese(siamese_model, margin=MARGIN)\n","trunk_model.summary()\n","model.summary()"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Model: \"conv_model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 121, 121, 64)      12352     \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 60, 60, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 53, 53, 128)       524416    \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 26, 26, 128)       0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 23, 23, 128)       262272    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 11, 11, 128)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 256)         524544    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 4096)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 2048)              8390656   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                65568     \n","=================================================================\n","Total params: 9,779,808\n","Trainable params: 9,779,808\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"Siamese_triplet_model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","in_anchor (InputLayer)          [(None, 128, 128, 3) 0                                            \n","__________________________________________________________________________________________________\n","in_positive (InputLayer)        [(None, 128, 128, 3) 0                                            \n","__________________________________________________________________________________________________\n","in_negative (InputLayer)        [(None, 128, 128, 3) 0                                            \n","__________________________________________________________________________________________________\n","Siamese_model_conv_model_2 (Mod (None,)              9779808     in_anchor[0][0]                  \n","                                                                 in_positive[0][0]                \n","                                                                 in_anchor[0][0]                  \n","                                                                 in_negative[0][0]                \n","__________________________________________________________________________________________________\n","subtract_1 (Subtract)           (None,)              0           Siamese_model_conv_model_2[1][0] \n","                                                                 Siamese_model_conv_model_2[2][0] \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None,)              0           subtract_1[0][0]                 \n","==================================================================================================\n","Total params: 9,779,808\n","Trainable params: 9,779,808\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pjsdV_yIc2Hr","colab_type":"text"},"source":["If there's a model to restore, we will try to restore weights"]},{"cell_type":"code","metadata":{"id":"zOzNhmkjc1Pw","colab_type":"code","outputId":"b1930e5b-a693-4c06-d82b-bc53b3639471","executionInfo":{"status":"ok","timestamp":1584542701058,"user_tz":-60,"elapsed":49972,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["if LOAD_FROM:\n","  if LOAD_FROM == \"save_dir\":\n","    LOAD_FROM = get_checkpoint_path()\n","  print(\"Load weights from\", LOAD_FROM)\n","  # model.load_weights(LOAD_FROM)\n","  # model = tf.keras.models.load_model(LOAD_FROM)\n","  trunk_model.load_weights(LOAD_FROM)\n","else:\n","  print(\"No weights to load\")"],"execution_count":42,"outputs":[{"output_type":"stream","text":["No weights to load\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nvrjm90Ficov","colab_type":"text"},"source":["## Prediction and evaluation models and functions"]},{"cell_type":"code","metadata":{"id":"MsjhioL9ij_Y","colab_type":"code","colab":{}},"source":["class NearestPredictor:\n","  def __init__(self, trunk_model, datas=([], [])):\n","    self.set_datas(datas)\n","    self.trunk_model = trunk_model\n","  \n","  def set_datas(self, datas):\n","    self.images, self.labels = datas\n","  \n","  def build(self):\n","    self.img_coords = self.trunk_model.predict(np.array(self.images))\n","\n","  def predict_in_datas(self, i):\n","    dists = [dist_fct(self.img_coords[i], coord) for coord in self.img_coords]\n","    dists[i] = max(dists) * 2 + 1\n","    return self.labels[np.argmin(dists)]\n","\n","  def predict(self, image):\n","    predict_coords = self.trunk_model.predict(np.array([image]))[0]\n","    dists = [dist_fct(predict_coords, coord) for coord in self.img_coords]\n","    return self.labels[np.argmin(dists)]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW1Z7O66h5JO","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_in_datas(predict_obj, ids_sample):\n","  good_results = [i for i in ids_sample if predict_obj.labels[i] == predict_obj.predict_in_datas(i)]\n","  return (len(good_results) / len(ids_sample), good_results)\n","\n","def evaluate_accuracy(predict_obj, datas, labels, ids_sample=None):\n","  if ids_sample == None:\n","    ids_sample = list(range(len(datas)))\n","  good_results = [i for i in ids_sample if labels[i] == predict_obj.predict(datas[i])]\n","  return (len(good_results) / len(ids_sample), good_results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D64gMg6W1wW1","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_train_datas():\n","  return evaluate_accuracy_in_datas(predict, train_datas_sample)\n","\n","def evaluate_accuracy_test_datas():\n","  return evaluate_accuracy_in_datas(predict_testing, test_datas_sample)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkwMelCGgdGa","colab_type":"code","colab":{}},"source":["def random_sample_homogeneous(labels, sample_size):\n","  nb_cls = max(labels)+1\n","  ids_of_cls = [[] for _ in range(nb_cls)]\n","  for i, cls in enumerate(labels):\n","    ids_of_cls[cls].append(i)\n","  \n","  per_cls = sample_size // nb_cls\n","  taken, not_taken = [], []\n","  for ids in ids_of_cls:\n","    random.shuffle(ids)\n","    taken.extend(ids[:per_cls])\n","    not_taken.extend(ids[per_cls:])\n","  if len(taken) < sample_size:\n","    taken.extend(random.sample(not_taken, sample_size-len(taken)))\n","  return taken\n","\n","predict = NearestPredictor(trunk_model, (train_images, train_labels))\n","predict_testing = NearestPredictor(trunk_model, (test_images, test_labels))\n","predict_base = NearestPredictor(trunk_model, (test_base_images, test_base_labels))\n","\n","# train_datas_sample = random.sample(list(range(len(train_labels))), ACCURACY_SAMPLE_SIZE)\n","# test_datas_sample = random.sample(list(range(len(test_labels))), ACCURACY_SAMPLE_SIZE)\n","train_datas_sample = random_sample_homogeneous(train_labels, len(train_labels))\n","test_datas_sample = random_sample_homogeneous(test_labels, len(test_labels))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RofKjrjsqobE","colab_type":"text"},"source":["## Monitor and prepare training"]},{"cell_type":"code","metadata":{"id":"0xLiPakxzG6k","colab_type":"code","colab":{}},"source":["def triplet_loss(y_true, y_pred):\n","  return K.mean(y_pred)\n","\n","def accuracy(y_true, y_pred):\n","  return K.mean(y_pred[:] <= 0.0000001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEGo4MPargI2","colab_type":"text"},"source":["Training callbacks"]},{"cell_type":"code","metadata":{"id":"8ze4leqBq6s9","colab_type":"code","colab":{}},"source":["class EpochStdoutLoggerCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_begin(self, epoch, logs):\n","    self.time = time.time()\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    epoch_time = time.time() - self.time\n","    print(\"Epoch {}/{} finished in {}m {}s | loss: {:.5f} - accuracy: {:.5f}\".format(\n","      epoch+1, NB_EPOCHS, int(epoch_time) // 60, int(epoch_time) % 60, logs['loss'], logs['accuracy']\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPc_qdnGq-t3","colab_type":"code","colab":{}},"source":["class AccuracyCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, epoch_interval, accuracies=None):\n","    super().__init__()\n","    self.epoch_interval = epoch_interval\n","    self.accuracies = accuracies\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    if epoch % self.epoch_interval == self.epoch_interval - 1:\n","      predict.build()\n","      acc, good_datas = evaluate_accuracy_in_datas(predict, train_datas_sample)\n","      print(\"\\nAccuracy : {}%\".format(acc * 100))\n","      if self.accuracies:\n","        self.accuracies.append(acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBJh91_7Egrc","colab_type":"code","colab":{}},"source":["class SaveTrunkCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, trunk, path, load_weights_on_restart=False, batch_interval=None):\n","    self.trunk = trunk\n","    self.path = path\n","    self.load_weights_on_restart = load_weights_on_restart\n","    self.batch_interval = batch_interval\n","  \n","  def save(self):\n","    self.trunk.save_weights(self.path)\n","\n","  def on_train_begin(self, logs=None):\n","    if (self.load_weights_on_restart and os.path.exists(self.path)):\n","      self.trunk.load_weights(self.path)\n","  \n","  def on_train_batch_end(self, batch, logs=None):\n","    if self.batch_interval and batch % self.batch_interval == 0:\n","      self.save()\n","  \n","  def on_epoch_end(self, epoch, logs=None):\n","    self.save()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Hp92KN7U6Mv","colab_type":"text"},"source":["## Training phase"]},{"cell_type":"code","metadata":{"id":"nNxs015TxoRJ","colab_type":"code","colab":{}},"source":["model.compile(\n","    # optimizer=tf.optimizers.Adam(learning_rate=0.0000000001), # LEARNING_RATE\n","    # optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.00000001),\n","    optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.001),\n","    loss=triplet_loss,\n","    metrics=[accuracy]\n",")\n","\n","hists, accuracies = [], []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"34mW85aOyD4B","colab_type":"code","colab":{}},"source":["# !rm -R logs/*\n","# %tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGNJ0OZextKo","colab_type":"code","outputId":"c8e722a7-0e44-438a-81fe-b9efdd7ab721","colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"status":"error","timestamp":1584542711614,"user_tz":-60,"elapsed":60318,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}}},"source":["triplet_generator = create_triplet_generator(\n","    train_images, train_labels, trunk_model,\n","    get_triplets_dists,\n","    # get_triplets_random,\n","    BATCH_SIZE\n",")\n","\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","checkpoint_path = get_checkpoint_path(suffix=\"_5\")\n","print(\"Saving weights at\", checkpoint_path)\n","\n","callbacks = [\n","  SaveTrunkCallback(trunk_model, checkpoint_path, load_weights_on_restart=True, batch_interval=1000),\n","  # EpochStdoutLoggerCallback(),\n","  AccuracyCallback(1, accuracies),\n","  # keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n","]\n","\n","r = model.fit_generator(\n","  generator=triplet_generator,\n","  epochs = NB_EPOCHS,\n","  steps_per_epoch = int(len(train_images) * TRIPLETS_PER_IMAGE / BATCH_SIZE),\n","  callbacks=callbacks,\n","  # verbose=0,\n","  # validation_data=(x_test, y_test)\n",")\n","hists.append(r)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Saving weights at drive/My Drive/ml/weights/oneshot_fishes/checkpoint_conv_2/weights_5.hdf5\n","Epoch 1/100\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-52f7a8cadf9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mTRIPLETS_PER_IMAGE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;31m# verbose=0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# validation_data=(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"8OM-CAjmeCSV","colab_type":"text"},"source":["## Functions to compute / plot stats about trained models"]},{"cell_type":"code","metadata":{"id":"3HYfUdZVeBs8","colab_type":"code","colab":{}},"source":["def eval_dists_on_sample(predict_obj, ids_sample):\n","  same_dists, diff_dists = [], []\n","\n","  for i in ids_sample:\n","    i_cls, i_coord = predict_obj.labels[i], predict_obj.img_coords[i]\n","    for j, j_coord in enumerate(predict_obj.img_coords):\n","      if i != j:\n","        if i_cls == predict_obj.labels[j]:\n","          same_dists.append(dist_fct(i_coord, j_coord))\n","        else:\n","          diff_dists.append(dist_fct(i_coord, j_coord))\n","  return same_dists, diff_dists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brEHu0gle5Dg","colab_type":"code","colab":{}},"source":["def plot_hist(arrs):\n","  plt.figure(figsize=(12,5))\n","  plt.hist(arrs,\n","    bins = 60,\n","    color = ['blue', '#D72F1A'],\n","    # edgecolor = 'black',\n","    label=[\"Same dists\", \"Diff dists\"],\n","    density=True\n","  )\n","  plt.legend(loc='upper right')\n","\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-m-84ILPFYx","colab_type":"code","colab":{}},"source":["def plot_results_per_class(good_results, labels, cls_names, sample=None):\n","  n_cls = len(cls_names)\n","  cls_success, cls_failed = [0]*n_cls, [0]*n_cls\n","\n","  if n_cls > 60:\n","    plt.figure(figsize=(35,6))\n","  else:\n","    plt.figure(figsize=(15,6))\n","\n","  is_success = [False] * len(labels)\n","  if sample:\n","    is_success = [None] * len(labels) # None stand for unused\n","    for i in sample:\n","      is_success[i] = False\n","  \n","  for i in good_results:\n","    is_success[i] = True\n","\n","  for i, succ in enumerate(is_success):\n","    if succ == True:\n","      cls_success[labels[i]] += 1\n","    elif succ == False:\n","      cls_failed[labels[i]] += 1\n","  \n","  ind = np.arange(n_cls)\n","  width = 0.8 # the width of the bars: can also be len(x) sequence\n","  rotation = 45 if n_cls < 60 else 90\n","\n","  p1 = plt.bar(ind, cls_success, width, color=\"#4CAF50\")\n","  p2 = plt.bar(ind, cls_failed, width, bottom=cls_success, color=\"#EF5350\")\n","\n","  plt.ylabel('Number of tests')\n","  plt.xlabel('Fish species')\n","  plt.title('Number of detection success and failure per fish species')\n","  plt.xticks(ind, cls_names, rotation=rotation)\n","  # plt.yticks(np.arange(0, 81, 10))\n","  plt.legend((p1[0], p2[0]), ('Success', 'Failed'))\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw6KfMEPU7-h","colab_type":"text"},"source":["## Display trained model stats"]},{"cell_type":"code","metadata":{"id":"l_TylF5AWw56","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"808b48ce-6929-4349-eb87-13df4f081bf7"},"source":["%%time\n","print(\"===== TRAINING STATS =====\")\n","predict.build()\n","same_dists, diff_dists = eval_dists_on_sample(predict, train_datas_sample)\n","accuracy, good_results = evaluate_accuracy_train_datas()\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","\n","plot_results_per_class(good_results, train_labels, train_cls_names, sample=train_datas_sample)\n","plot_hist([same_dists, diff_dists])\n","print(len(train_images), ACCURACY_SAMPLE_SIZE, ACCURACY_SAMPLE_SIZE * len(train_images))\n","print(len(same_dists), len(diff_dists))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["===== TRAINING STATS =====\n","\n","triplets computed 15 s (useful, unuseful) = (32300, 0) (100.00%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lKOpRm8wdIdr","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TESTING WITHIN SAME DATAS STATS =====\")\n","predict_testing.build()\n","same_dists, diff_dists = eval_dists_on_sample(predict_testing, test_datas_sample)\n","accuracy, good_results = evaluate_accuracy_test_datas()\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","\n","plot_results_per_class(good_results, test_labels, test_cls_names, sample=test_datas_sample)\n","plot_hist([same_dists, diff_dists])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFUfMeIYMKdE","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TESTING WITH DIFFERENT DATAS STATS =====\")\n","predict_base.build()\n","# same_dists, diff_dists = eval_dists_on_sample(predict_base, test_datas_sample)\n","\n","accuracy, good_results = evaluate_accuracy(predict_base, test_images, test_labels)\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","plot_results_per_class(good_results, test_labels, test_cls_names)\n","\n","# print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","# print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","# plot_hist([same_dists, diff_dists])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R_SwHLuYwA8A","colab":{}},"source":["# %matplotlib inline\n","\n","# for _ in range(3):\n","#     try:\n","#         pl.clf()\n","#         pl.plot(pd.Series(data=np.random.randn(100), index=i))\n","#         display.display(pl.gcf())\n","#         display.clear_output(wait=True)\n","#         time.sleep(1)\n","#     except KeyboardInterrupt:\n","#         break"],"execution_count":0,"outputs":[]}]}