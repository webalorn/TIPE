{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OneShot_characters.ipynb","provenance":[],"collapsed_sections":["Dg26SMGNUthi","p5d2tWuMVHfA","RofKjrjsqobE"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m0w2omm1UJ9S","colab_type":"text"},"source":["# One-shot learning implementation for character recognition\n","Using the [omniglot dataset](https://github.com/brendenlake/omniglot/tree/master/python)"]},{"cell_type":"markdown","metadata":{"id":"FhnOva4D5GZQ","colab_type":"text"},"source":["### TODO\n","- entrainer sans negatif ? \n","- data augmentation ?\n","- comparer avec et sans\n","  - batch norm\n","  - batch norm au début\n","  - dropout\n","  - regul"]},{"cell_type":"markdown","metadata":{"id":"4mTAcOcdUd93","colab_type":"text"},"source":["## Setup phase\n","We install packages, make all imports, configure modules and download dataset"]},{"cell_type":"code","metadata":{"id":"Gph_E9QuwQS4","colab_type":"code","outputId":"7e708cd7-667b-4cc9-84fe-a3f8e727fbde","executionInfo":{"status":"ok","timestamp":1584546254093,"user_tz":-60,"elapsed":3445,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":117,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I1246o-3RW3v","colab_type":"code","outputId":"7684847c-3011-4fc9-e7e1-8a8f91bc524f","executionInfo":{"status":"ok","timestamp":1584546261237,"user_tz":-60,"elapsed":10564,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["%%bash\n","pip install -q pyyaml\n","pip install tensorflow==2.0.0-beta1\n","pip install -q tensorflow-gpu==2.0.0-beta1"],"execution_count":118,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.24.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.34.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.18.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.9.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (45.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DHAExJYpSHIb","colab_type":"code","outputId":"cbd01751-1263-4c6e-d376-273da151a651","executionInfo":{"status":"ok","timestamp":1584546261238,"user_tz":-60,"elapsed":10541,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["%%bash\n","git clone https://github.com/brendenlake/omniglot\n","mkdir datas\n","unzip -q omniglot/python/images_background.zip -d datas\n","unzip -q omniglot/python/images_evaluation.zip -d datas\n","# rm -R omniglot"],"execution_count":119,"outputs":[{"output_type":"stream","text":["fatal: destination path 'omniglot' already exists and is not an empty directory.\n","mkdir: cannot create directory ‘datas’: File exists\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [unzip -q ]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [omniglot/]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [python/im]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ages_eval]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [uation.zi]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [p -d data]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [s]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [# rm -R o]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [mniglot]\n","replace datas/images_background/Alphabet_of_the_Magi/character01/0709_01.png? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n","(EOF or read error, treating as \"[N]one\" ...)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dy0U2QJJRYmd","colab_type":"code","outputId":"7b46a2b0-4480-4548-cace-96e737344e4f","executionInfo":{"status":"ok","timestamp":1584546261238,"user_tz":-60,"elapsed":10505,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["%load_ext tensorboard\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras import datasets, layers, models\n","from tensorboard import notebook\n","from keras import backend as K\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pl\n","import pandas as pd\n","import numpy as np\n","import os, datetime, time, math, pathlib, itertools, random\n","\n","keras = tf.keras\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","print(tf.version.VERSION)\n","print(tf.keras.__version__)\n","print(\"GPU Available: \", tf.test.is_gpu_available())"],"execution_count":120,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n","2.0.0-beta1\n","2.2.4-tf\n","GPU Available:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_af6yejxduUq","colab_type":"text"},"source":["## Constants\n","This part will define how to build, train, and evaluate the model"]},{"cell_type":"code","metadata":{"id":"l16hgYf3UHel","colab_type":"code","cellView":"both","outputId":"b63e1408-3f73-4dfe-f903-452d622e6043","executionInfo":{"status":"ok","timestamp":1584546261238,"user_tz":-60,"elapsed":10474,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@markdown ## Data paths\n","DIR_TRAIN = \"datas/images_background\" #@param {type:\"string\"}\n","DIR_TEST = \"datas/images_evaluation\" #@param {type:\"string\"}\n","LOAD_FROM = \"\"  #@param [\"\", \"save_dir\", \"/content/drive/My Drive/ml/weights/oneshot_chars/checkpoint_dense/weights.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_chars/checkpoint_conv/weights.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_chars/checkpoint_conv2/weights.hdf5\"] {allow-input: true}\n","CHECKPOINTS_DIR = \"drive/My Drive/ml/weights/oneshot_chars\"\n","checkpoint_dir_name = \"checkpoint_\" + str(int(time.time()))\n","\n","\n","#@markdown ## Model configuration\n","MODEL_TYPE = \"conv3\" #@param [\"same\", \"linear\",\"dense\", \"conv\", \"conv2\", \"conv3\"]\n","IMG_SIDE = 100 #@param {type:\"slider\", min:10, max:150, step:1}\n","IMG_SHAPE = (IMG_SIDE, IMG_SIDE)\n","\n","#@markdown ## Training configuration\n","NB_EPOCHS = 100 #@param {type:\"number\"}\n","BATCH_SIZE = 32 #@param {type:\"number\"}\n","TRIPLETS_PER_IMAGE = 10 #@param {type:\"number\"}\n","LEARNING_RATE = 0.000000001 #@param {type:\"number\"}\n","MARGIN = 1 #@param {type:\"number\"}\n","\n","#@markdown ## Evaluation configuration\n","ACCURACY_SAMPLE_SIZE = 400 #@param {type:\"number\"}\n","\n","checkpoint_dir_name = \"checkpoint_\" + MODEL_TYPE\n","print(\"Saving in {} for this session\".format(checkpoint_dir_name))\n","if LOAD_FROM:\n","  print(\"Loading weights from checkpoint {}\".format(LOAD_FROM))"],"execution_count":121,"outputs":[{"output_type":"stream","text":["Saving in checkpoint_conv3 for this session\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dg26SMGNUthi","colab_type":"text"},"source":["## General code\n","Helper functions"]},{"cell_type":"code","metadata":{"id":"435-vCZcqPyc","colab_type":"code","colab":{}},"source":["def getRandomIds(dataset, nMax=1000):\n","\tids = list(range(len(dataset[0][0])-1))\n","\trandom.shuffle(ids)\n","\tids = ids[:nMax]\n","\treturn ids + [i+1 for i in ids]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG591ol1s-zz","colab_type":"code","colab":{}},"source":["  def dist_fct(x, y):\n","    return np.sqrt(np.sum((x-y)**2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7BKheGyefQY","colab_type":"code","colab":{}},"source":["def get_checkpoint_path(suffix=\"\"):\n","  os.makedirs(os.path.join(CHECKPOINTS_DIR, checkpoint_dir_name), exist_ok=True)\n","  return os.path.join(\n","    CHECKPOINTS_DIR,\n","    checkpoint_dir_name,\n","    \"weights\" + suffix + \".hdf5\"\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBDI3NQtQ9u","colab_type":"code","colab":{}},"source":["class Timer():\n","  def __init__(self, to_int = True):\n","    self.t = time.time()\n","    self.to_int = to_int\n","  \n","  def get(self, reset=True):\n","    t2 = time.time()\n","    d = t2 - self.t\n","    if self.to_int:\n","      d = int(d)\n","    if reset:\n","      self.t = t2\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuTeqs4BqpS6","colab_type":"code","colab":{}},"source":["def plot_history(histories, key='binary_crossentropy'):\n","  plt.figure(figsize=(16,10))\n","\n","  for name, history in histories:\n","    val = plt.plot(history.epoch, history.history['val_'+key],\n","                   '--', label=name.title()+' Val')\n","    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n","             label=name.title()+' Train')\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(key.replace('_',' ').title())\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYCmvp11iYp_","colab_type":"code","colab":{}},"source":["def show_image(image):\n","\tplt.imshow(image)\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5d2tWuMVHfA","colab_type":"text"},"source":["## Import datas and pre-processing"]},{"cell_type":"code","metadata":{"id":"gLyuKF73v_Hh","colab_type":"code","colab":{}},"source":["def preprocess_image(image):\n","  image = tf.image.decode_image(image)\n","  image = tf.image.resize(image, IMG_SHAPE)\n","  image = (255.0 - image.numpy().astype(float) ) / 255.0\n","  image = image.reshape(IMG_SHAPE)\n","  return image\n","\n","def load_and_preprocess_image(img_path):\n","  return preprocess_image(tf.io.read_file(str(img_path)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5Ha_hh5q9Sq","colab_type":"code","colab":{}},"source":["def getDatasPaths(dir_path, max_alphabet=None):\n","  charSets = list(os.listdir(dir_path))\n","  if max_alphabet:\n","    charSets = max_alphabet[:max_alphabet]\n","  charClasses = [[(d, c) for c in os.listdir(os.path.join(dir_path, d))] for d in charSets]\n","  charClasses = list(itertools.chain(*charClasses))\n","  charDirs = [os.path.join(dir_path, d, c) for d, c in charClasses]\n","  imgInfos = [\n","      [(os.path.join(cls_path, img_f_name), label) for img_f_name in os.listdir(cls_path)]\n","      for cls_path, label in zip(charDirs, list(range(len(charDirs))))\n","  ]\n","  imgInfos = list(itertools.chain(*imgInfos))\n","  paths, labels = [[el[i] for el in imgInfos] for i in range(2)]\n","  cls_names = [\" - \".join(cls) for cls in charClasses]\n","  return paths, labels, cls_names # len : nb images | nb images | nb classes\n","\n","def loadDatas(dir_path, max_alphabet=None):\n","  paths, labels, cls_names = getDatasPaths(dir_path, max_alphabet=max_alphabet)\n","  images_datas = [load_and_preprocess_image(img_path) for img_path in paths]\n","      \n","  return images_datas, labels, cls_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0jvq2J8UItF","colab_type":"text"},"source":["Now, we read all the datas"]},{"cell_type":"code","metadata":{"id":"EXXibU93QljQ","colab_type":"code","colab":{}},"source":["try:\n","  _ = train_images\n","except:\n","  train_images, train_labels, train_cls_names = loadDatas(DIR_TRAIN)\n","  test_images, test_labels, test_cls_names = loadDatas(DIR_TEST)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LnO97_TKW26s","colab_type":"text"},"source":["## Functions to feed datas to the network"]},{"cell_type":"markdown","metadata":{"id":"7Z7tX_tCazT7","colab_type":"text"},"source":["First, helper functions to :\n","- get all images classed by label"]},{"cell_type":"code","metadata":{"id":"G4utCJ0DW6si","colab_type":"code","colab":{}},"source":["def get_ids_per_cls(labels):\n","  ids_per_cls = []\n","  for i in range(len(labels)):\n","    while len(ids_per_cls) <= labels[i]:\n","      ids_per_cls.append([])\n","    ids_per_cls[labels[i]].append(i)\n","  return ids_per_cls\n","\n","def sort_by_distance(l, anchor, only_ids=True):\n","  l2 = [(dist_fct(el, anchor), i) for i, el in enumerate(l)]\n","  l2.sort()\n","  if only_ids:\n","    return [i for d, i in l2]\n","  return [l[i] for d, i in l2]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USG4PW7hbDJ9","colab_type":"text"},"source":["The following function select random triplets to train the NN"]},{"cell_type":"code","metadata":{"id":"eWsnArbWa-px","colab_type":"code","colab":{}},"source":["def get_triplets_random(images, labels, trunk_model):\n","  nb_images = len(images)\n","  ids_per_cls = get_ids_per_cls(labels)\n","  triplets = []\n","  \n","  for i_anchor in range(nb_images):\n","    same_cls = [i for i in ids_per_cls[labels[i_anchor]] if i != i_anchor]\n","    for _ in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = random.choice(same_cls), i_anchor\n","      while labels[i_negative] == labels[i_anchor]:\n","        i_negative = random.randint(0, nb_images-1)\n","      triplets.append([i_anchor, i_positive, i_negative])\n","  \n","  return triplets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qq90WWVLDrJ","colab_type":"text"},"source":["This functions try to select triplets better than random ones. At first, we run the NN on all examples. Then, we try to select triplets with a positive far from the anchor, and a negative close to it."]},{"cell_type":"code","metadata":{"id":"xtaHxTLzLCzT","colab_type":"code","colab":{}},"source":["# %%time\n","NB_CENTERS_PER_IMAGE = 3\n","# FACT_RANDOM = 3\n","# FACT_RANDOM_POSITIVE = 2\n","\n","def get_triplets_dists(images, labels, trunk_model):\n","  timer = Timer()\n","  coords = trunk_model.predict(np.array(images))\n","  ids_per_cls = get_ids_per_cls(labels)\n","  centers = [np.mean([coords[i] for i in ids_per_cls[lab]], axis=0) for lab in range(len(ids_per_cls))]\n","\n","  centers_away_from_cls = [[] for _ in range(len(ids_per_cls))]\n","  for i_cls in range(len(ids_per_cls)):\n","    centers_sorted = sort_by_distance(centers, centers[i_cls])\n","    centers_sorted = [i_center for i_center in centers_sorted if i_center != i_cls][:NB_CENTERS_PER_IMAGE]\n","    centers_away_from_cls[i_cls] = centers_sorted\n","  \n","  triplets = []\n","  useful, unuseful = 0, 0\n","  for anchor in range(len(images)):\n","    same_cls = [i for i in ids_per_cls[labels[anchor]] if i != anchor] or [anchor]\n","    # positives = [random.choice(same_cls) for _ in range(TRIPLETS_PER_IMAGE)]\n","    positives_order = sort_by_distance([coords[i] for i in same_cls], anchor)[::-1]\n","    positives = [same_cls[i] for i in positives_order]\n","    # random.shuffle(positives)\n","\n","    centers_taken = centers_away_from_cls[labels[anchor]]\n","    negatives = list(itertools.chain(*[ids_per_cls[i_cls] for i_cls in centers_taken]))\n","    negatives_order = sort_by_distance([coords[i] for i in negatives], anchor)\n","    negatives = [negatives[i] for i in negatives_order]\n","    # negatives = negatives[:FACT_RANDOM*TRIPLETS_PER_IMAGE]\n","    # random.shuffle(negatives)\n","\n","    for i in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = i%len(positives), i%len(negatives)\n","      dist_diff = dist_fct(coords[anchor], coords[positives[i_positive]]) - dist_fct(coords[anchor], coords[negatives[i_negative]])\n","      if dist_diff + MARGIN < 0:\n","        unuseful += 1\n","      else:\n","        useful += 1\n","        triplets.append([anchor, positives[i_positive], negatives[i_negative]])\n","      # print(triplets[-1], [labels[j] for j in triplets[-1]])\n","  \n","  print(\"\\ntriplets computed\", timer.get(), \"s\", \"(useful, unuseful) =\", (useful, unuseful), \"({:.2f}%)\".format(useful / (useful + unuseful) * 100))\n","\n","  return triplets\n","\n","# triplets = get_triplets_dists(train_images, train_labels, trunk_model) \n","\n","# %%time\n","# NB_CENTERS_PER_IMAGE = 3\n","# # FACT_RANDOM = 3\n","# # FACT_RANDOM_POSITIVE = 2\n","\n","# def get_triplets_dists(images, labels, trunk_model):\n","#   timer = Timer()\n","#   coords = trunk_model.predict(np.array(images))\n","#   ids_per_cls = get_ids_per_cls(labels)\n","#   centers = [np.mean([coords[i] for i in ids_per_cls[lab]]) for lab in range(len(ids_per_cls))]\n","\n","#   centers_away_from_cls = [[] for _ in range(len(ids_per_cls))]\n","#   for i_cls in range(len(ids_per_cls)):\n","#     centers_sorted = sort_by_distance(centers, centers[i_cls])\n","#     centers_sorted = [i_center for i_center in centers_sorted if i_center != i_cls][:NB_CENTERS_PER_IMAGE]\n","#     centers_away_from_cls[i_cls] = centers_sorted\n","  \n","#   triplets = []\n","#   useful, unuseful = 0, 0\n","#   for anchor in range(len(images)):\n","#     same_cls = [i for i in ids_per_cls[labels[anchor]] if i != anchor]\n","#     # positives = [random.choice(same_cls) for _ in range(TRIPLETS_PER_IMAGE)]\n","#     positives_order = sort_by_distance([coords[i] for i in same_cls], anchor)[::-1]\n","#     positives = [same_cls[i] for i in positives_order]\n","#     # random.shuffle(positives)\n","\n","#     centers_taken = centers_away_from_cls[labels[anchor]]\n","#     negatives = list(itertools.chain(*[ids_per_cls[i_cls] for i_cls in centers_taken]))\n","#     negatives_order = sort_by_distance([coords[i] for i in negatives], anchor)\n","#     negatives = [negatives[i] for i in negatives_order]\n","#     # negatives = negatives[:FACT_RANDOM*TRIPLETS_PER_IMAGE]\n","#     # random.shuffle(negatives)\n","\n","#     for i in range(TRIPLETS_PER_IMAGE):\n","#       if dist_fct(coords[anchor], coords[positives[i]]) - dist_fct(coords[anchor], coords[negatives[i]]) + MARGIN < 0:\n","#         unuseful += 1\n","#       else:\n","#         useful += 1\n","#         triplets.append([anchor, positives[i], negatives[i]])\n","#       # print(triplets[-1], [labels[j] for j in triplets[-1]])\n","  \n","#   print(\"\\ntriplets computed\", timer.get(), \"s\", \"(useful, unuseful) =\", (useful, unuseful), \"({:.2f}%)\".format(useful / (useful + unuseful) * 100))\n","\n","#   return triplets\n","\n","# # triplets = get_triplets_dists(train_images, train_labels, trunk_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw5VslCJbIRI","colab_type":"text"},"source":["Now, we need a function to generate triplets during the training process. This function will be called by `fit_generator`"]},{"cell_type":"code","metadata":{"id":"zIHip7tWbCFy","colab_type":"code","colab":{}},"source":["def create_triplet_generator(images, labels, trunk_model, triplets_getter, batch_size):\n","  triplets = []\n","  cur_triplet = 0\n","  while True:\n","    if cur_triplet + batch_size > len(triplets):\n","      triplets = triplets_getter(images, labels, trunk_model)\n","      random.shuffle(triplets)\n","      cur_triplet = 0\n","    \n","    yield (\n","      [ np.array([images[triplets[cur_triplet + i_triplet][i_in]] for i_triplet in range(batch_size)]) for i_in in range(3)],\n","      [0] * batch_size\n","    )\n","\n","    cur_triplet += batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vx1Ob-kU4JD","colab_type":"text"},"source":["## Model definition"]},{"cell_type":"code","metadata":{"id":"enIVJB3exhWY","colab_type":"code","colab":{}},"source":["def create_same_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","  ], name=\"same_model\")\n","  return model\n","\n","def create_linear_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(),\n","    layers.Flatten(),\n","    layers.Dense(32, activation='relu'),\n","  ], name=\"linear_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02rcAoUjUziy","colab_type":"text"},"source":["Dense part of a siasme network"]},{"cell_type":"code","metadata":{"id":"dX6A30xTUkrf","colab_type":"code","colab":{}},"source":["def create_dense_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","    layers.Flatten(),\n","    layers.Dense(300, activation='relu'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(100, activation='relu'),\n","    # layers.Dropout(0.2),\n","    layers.Dense(8, activation='softmax')\n","  ], name=\"dense_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeQMkXBnthF-","colab_type":"text"},"source":["Convolutional trunk part of siasme network"]},{"cell_type":"code","metadata":{"id":"hoMNl6P8tktt","colab_type":"code","colab":{}},"source":["def create_conv_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.BatchNormalization(),\n","    layers.Reshape(IMG_SHAPE + (1,)),\n","\n","    layers.Conv2D(20, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(40, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(300, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(100, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtWd0_9SCv0O","colab_type":"code","colab":{}},"source":["def create_conv_2_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(),\n","    layers.Reshape(IMG_SHAPE + (1,)),\n","\n","    layers.Conv2D(64, (8, 8), activation='relu'), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (8, 8), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Flatten(),\n","    layers.Dense(512, activation='tanh'),\n","    # layers.Dense(32 , activation='tanh'),\n","\n","    # layers.Flatten(),\n","    # layers.Dense(300, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(100, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model_2\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQWiGR40NXpJ","colab_type":"code","colab":{}},"source":["def create_conv_3_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    # layers.BatchNormalization(), # TODO ? \n","    layers.Reshape(IMG_SHAPE + (1,)),\n","\n","    layers.Conv2D(64, (8, 8), activation='relu'), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (8, 8), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu'),\n","\n","    layers.Flatten(),\n","    layers.Dense(2048, activation='tanh'), # TODO : 4096 ?\n","    layers.Dense(16, activation='tanh'), # TODO : 64 ?\n","  ], name=\"conv_model_3\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlCDXXUVU_Mp","colab_type":"text"},"source":["The first function, given a model, create a siamese NN with this model as the common part. The second create a model with two times the same siamese network to compute triplet loss."]},{"cell_type":"code","metadata":{"id":"OoW1fHWqU-SL","colab_type":"code","colab":{}},"source":["def create_siamese(trunk_model):\n","  inputs = [layers.Input(IMG_SHAPE) for _ in range(2)]\n","  parts = [trunk_model(inTensor) for inTensor in inputs]\n","  # diff = layers.subtract(parts)\n","  # out = layers.Lambda(lambda x : tf.reduce_sum(x**2, axis=(1,)))(diff)\n","  # out_sqrt = layers.Lambda(lambda x : tf.sqrt(x))(out)\n","  out_sqrt = tf.sqrt(tf.reduce_sum((parts[1]-parts[0])**2, axis=(1,)))\n","  return keras.models.Model(inputs=inputs, outputs=out_sqrt, name=\"Siamese_model\"+\"_\"+trunk_model.name)\n","\n","def create_triplet_siamese(siamese_model, margin=1.0):\n","  in_anchor, in_positive, in_negative = [layers.Input(IMG_SHAPE, name=name) for name in [\"in_anchor\", \"in_positive\", \"in_negative\"]]\n","  positive_dist = siamese_model([in_anchor, in_positive])\n","  negative_dist = siamese_model([in_anchor, in_negative])\n","\n","  dist = layers.subtract([positive_dist, negative_dist])\n","  if margin:\n","    dist = layers.Lambda(lambda x : tf.maximum(x + margin, 0.0))(dist)\n","  # dist = layers.Lambda(lambda x : tf.square(x))(dist) # keep ?\n","  return keras.models.Model(inputs=[in_anchor, in_positive, in_negative], outputs=dist, name=\"Siamese_triplet_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO2W3mjSU_t3","colab_type":"text"},"source":["We will now choose the model we will use"]},{"cell_type":"code","metadata":{"id":"1sp3PzQwUvMw","colab_type":"code","outputId":"2e8b2c27-29c6-4750-fefd-49d662f44ed2","executionInfo":{"status":"ok","timestamp":1584546262428,"user_tz":-60,"elapsed":11313,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":935}},"source":["str2model = {\n","    \"same\" : create_same_trunk_model,\n","    \"linear\" : create_linear_trunk_model,\n","    \"dense\" : create_dense_trunk_model,\n","    \"conv\" : create_conv_trunk_model,\n","    \"conv2\" : create_conv_2_trunk_model,\n","    \"conv3\" : create_conv_3_trunk_model,\n","}\n","\n","trunk_model = str2model[MODEL_TYPE]()\n","siamese_model = create_siamese(trunk_model)\n","model = create_triplet_siamese(siamese_model, margin=MARGIN)\n","trunk_model.summary()\n","model.summary()"],"execution_count":141,"outputs":[{"output_type":"stream","text":["Model: \"conv_model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","reshape_3 (Reshape)          (None, 100, 100, 1)       0         \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 93, 93, 64)        4160      \n","_________________________________________________________________\n","max_pooling2d_9 (MaxPooling2 (None, 46, 46, 64)        0         \n","_________________________________________________________________\n","conv2d_13 (Conv2D)           (None, 39, 39, 128)       524416    \n","_________________________________________________________________\n","max_pooling2d_10 (MaxPooling (None, 19, 19, 128)       0         \n","_________________________________________________________________\n","conv2d_14 (Conv2D)           (None, 16, 16, 128)       262272    \n","_________________________________________________________________\n","max_pooling2d_11 (MaxPooling (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_15 (Conv2D)           (None, 5, 5, 256)         524544    \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 6400)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 2048)              13109248  \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 16)                32784     \n","=================================================================\n","Total params: 14,457,424\n","Trainable params: 14,457,424\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"Siamese_triplet_model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","in_anchor (InputLayer)          [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","in_positive (InputLayer)        [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","in_negative (InputLayer)        [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","Siamese_model_conv_model_3 (Mod (None,)              14457424    in_anchor[0][0]                  \n","                                                                 in_positive[0][0]                \n","                                                                 in_anchor[0][0]                  \n","                                                                 in_negative[0][0]                \n","__________________________________________________________________________________________________\n","subtract_3 (Subtract)           (None,)              0           Siamese_model_conv_model_3[1][0] \n","                                                                 Siamese_model_conv_model_3[2][0] \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None,)              0           subtract_3[0][0]                 \n","==================================================================================================\n","Total params: 14,457,424\n","Trainable params: 14,457,424\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pjsdV_yIc2Hr","colab_type":"text"},"source":["If there's a model to restore, we will try to restore weights"]},{"cell_type":"code","metadata":{"id":"zOzNhmkjc1Pw","colab_type":"code","outputId":"3e0d26ae-ec51-4ca2-ed0e-52610ad8923e","executionInfo":{"status":"ok","timestamp":1584546262429,"user_tz":-60,"elapsed":11283,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if LOAD_FROM:\n","  if LOAD_FROM == \"save_dir\":\n","    LOAD_FROM = get_checkpoint_path()\n","  print(\"Load weights from\", LOAD_FROM)\n","  # model.load_weights(LOAD_FROM)\n","  # model = tf.keras.models.load_model(LOAD_FROM)\n","  trunk_model.load_weights(LOAD_FROM)\n","else:\n","  print(\"No weights to load\")\n"],"execution_count":142,"outputs":[{"output_type":"stream","text":["No weights to load\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nvrjm90Ficov","colab_type":"text"},"source":["## Prediction and evaluation models and functions"]},{"cell_type":"code","metadata":{"id":"MsjhioL9ij_Y","colab_type":"code","colab":{}},"source":["class NearestPredictor:\n","  def __init__(self, trunk_model, datas=([], [])):\n","    self.set_datas(datas)\n","    self.trunk_model = trunk_model\n","  \n","  def set_datas(self, datas):\n","    self.images, self.labels = datas\n","  \n","  def build(self):\n","    self.img_coords = self.trunk_model.predict(np.array(self.images))\n","\n","  def predict_in_datas(self, i):\n","    dists = [dist_fct(self.img_coords[i], coord) for coord in self.img_coords]\n","    min_j = 0\n","    for j, coord in enumerate(self.img_coords):\n","      if j != i and (min_j == i or dists[j] < dists[min_j]):\n","        min_j = j\n","    return self.labels[min_j]\n","\n","  def predict(self, image):\n","    predict_coords = self.trunk_model.predict(np.array([image]))[0]\n","    dists = [dist_fct(predict_coords, coord) for coord in self.img_coords]\n","    min_j = 0\n","    for j, coord in enumerate(self.img_coords):\n","      if dists[j] < dists[min_j]:\n","        min_j = j\n","    return self.labels[min_j]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW1Z7O66h5JO","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_in_datas(predict_obj, ids_sample):\n","  predict_obj.build()\n","  return len([1 for i in ids_sample if predict_obj.labels[i] == predict_obj.predict_in_datas(i)]) / len(ids_sample)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D64gMg6W1wW1","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_train_datas():\n","  return evaluate_accuracy_in_datas(predict, train_datas_sample)\n","\n","def evaluate_accuracy_test_datas():\n","  return evaluate_accuracy_in_datas(predict_testing, test_datas_sample)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkwMelCGgdGa","colab_type":"code","colab":{}},"source":["predict = NearestPredictor(trunk_model, (train_images, train_labels))\n","predict_testing = NearestPredictor(trunk_model, (test_images, test_labels))\n","train_datas_sample = random.sample(list(range(len(train_labels))), ACCURACY_SAMPLE_SIZE)\n","test_datas_sample = random.sample(list(range(len(test_labels))), ACCURACY_SAMPLE_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RofKjrjsqobE","colab_type":"text"},"source":["## Monitor and prepare training"]},{"cell_type":"code","metadata":{"id":"0xLiPakxzG6k","colab_type":"code","colab":{}},"source":["def triplet_loss(y_true, y_pred):\n","  return K.mean(y_pred)\n","\n","def accuracy(y_true, y_pred):\n","  return K.mean(y_pred[:] <= 0.0000001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEGo4MPargI2","colab_type":"text"},"source":["Training callbacks"]},{"cell_type":"code","metadata":{"id":"8ze4leqBq6s9","colab_type":"code","colab":{}},"source":["class EpochStdoutLoggerCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_begin(self, epoch, logs):\n","    self.time = time.time()\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    epoch_time = time.time() - self.time\n","    print(\"Epoch {}/{} finished in {}m {}s | loss: {:.5f} - accuracy: {:.5f}\".format(\n","      epoch+1, NB_EPOCHS, int(epoch_time) // 60, int(epoch_time) % 60, logs['loss'], logs['accuracy']\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPc_qdnGq-t3","colab_type":"code","colab":{}},"source":["class AccuracyCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, epoch_interval, accuracies=None):\n","    super().__init__()\n","    self.epoch_interval = epoch_interval\n","    self.accuracies = accuracies\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    if epoch % self.epoch_interval == self.epoch_interval - 1:\n","      acc = evaluate_accuracy_train_datas()\n","      print(\"\\nAccuracy : {}%\".format(acc * 100))\n","      if self.accuracies:\n","        self.accuracies.append(acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBJh91_7Egrc","colab_type":"code","colab":{}},"source":["class SaveTrunkCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, trunk, path, load_weights_on_restart=False, batch_interval=None):\n","    self.trunk = trunk\n","    self.path = path\n","    self.load_weights_on_restart = load_weights_on_restart\n","    self.batch_interval = batch_interval\n","  \n","  def save(self):\n","    self.trunk.save_weights(self.path)\n","\n","  def on_train_begin(self, logs=None):\n","    if (self.load_weights_on_restart and os.path.exists(self.path)):\n","      self.trunk.load_weights(self.path)\n","  \n","  def on_train_batch_end(self, batch, logs=None):\n","    if self.batch_interval and batch % self.batch_interval == 0:\n","      self.save()\n","  \n","  def on_epoch_end(self, epoch, logs=None):\n","    self.save()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Hp92KN7U6Mv","colab_type":"text"},"source":["## Training phase"]},{"cell_type":"code","metadata":{"id":"nNxs015TxoRJ","colab_type":"code","colab":{}},"source":["model.compile(\n","    # optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), # 0.000000001\n","    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n","    loss=triplet_loss,\n","    metrics=[accuracy] # TODO : This doesn't give the accuracy of the classifier itself\n",")\n","\n","hists, accuracies = [], []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"34mW85aOyD4B","colab_type":"code","colab":{}},"source":["# !rm -R logs/*\n","# %tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGNJ0OZextKo","colab_type":"code","outputId":"0d999a30-93e9-4bb4-e534-b62310736bf0","executionInfo":{"status":"error","timestamp":1584537473405,"user_tz":-60,"elapsed":784516,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisQmAW9W19prkSZCvq-wr-mLHfWOyt2otaM5osa_Q=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time\n","%matplotlib inline\n","triplet_generator = create_triplet_generator(\n","    train_images, train_labels, trunk_model,\n","    get_triplets_dists,\n","    BATCH_SIZE\n",")\n","\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","checkpoint_path = get_checkpoint_path(suffix=\"_0\")\n","print(\"Saving weights at\", checkpoint_path)\n","\n","callbacks = [\n","  SaveTrunkCallback(trunk_model, checkpoint_path, load_weights_on_restart=False, batch_interval=1000),\n","  # EpochStdoutLoggerCallback(),\n","  AccuracyCallback(1, accuracies),\n","  # keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n","]\n","\n","r = model.fit_generator(\n","    generator=triplet_generator,\n","    epochs = NB_EPOCHS,\n","    steps_per_epoch = int(len(train_images) * TRIPLETS_PER_IMAGE / BATCH_SIZE),\n","    callbacks=callbacks,\n","    # verbose=0,\n","  # validation_data=(x_test, y_test)\n",")\n","hists.append(r)\n","\n","# TODO : valiation ; data augmentation ; get_triplet -> pas random"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saving weights at drive/My Drive/ml/weights/oneshot_chars/checkpoint_conv3/weights_0.hdf5\n","Epoch 1/100\n","\n","triplets computed 43 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","triplets computed 73 s (useful, unuseful) = (192800, 0) (100.00%)\n","   1/6025 [..............................] - ETA: 129:24:18 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.700488). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 85 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1510s 251ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.900882). Check your callbacks.\n","   1/6025 [..............................] - ETA: 2:00:45 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.450470). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 85 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1436s 238ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.835067). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:53:02 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.417562). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 87 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1430s 237ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.810345). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:50:19 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.405211). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 86 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1433s 238ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.758810). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:45:21 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.379434). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 91 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1432s 238ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.734552). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:43:08 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.367304). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 83 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1439s 239ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.854173). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:55:45 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.427116). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 84 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1438s 239ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.816114). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:51:04 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.408085). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 83 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1442s 239ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.730573). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:41:56 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.365314). Check your callbacks.\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n","triplets computed 84 s (useful, unuseful) = (192800, 0) (100.00%)\n","\n","Accuracy : 0.0%\n","6025/6025 [==============================] - 1447s 240ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.818321). Check your callbacks.\n","   1/6025 [..............................] - ETA: 1:51:21 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.409190). Check your callbacks.\n","2783/6025 [============>.................] - ETA: 11:47 - loss: 0.0000e+00 - accuracy: 1.0000Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8OM-CAjmeCSV","colab_type":"text"},"source":["## Functions to compute / plot stats about trained models"]},{"cell_type":"code","metadata":{"id":"3HYfUdZVeBs8","colab_type":"code","colab":{}},"source":["def eval_dists_on_sample(predict_obj, ids_sample):\n","  predict_obj.build()\n","  same_dists, diff_dists = [], []\n","\n","  for i in ids_sample:\n","    i_cls, i_coord = predict_obj.labels[i], predict_obj.img_coords[i]\n","    for j, j_coord in enumerate(predict_obj.img_coords):\n","      if i != j:\n","        if i_cls == predict_obj.labels[j]:\n","          same_dists.append(dist_fct(i_coord, j_coord))\n","        else:\n","          diff_dists.append(dist_fct(i_coord, j_coord))\n","  return same_dists, diff_dists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brEHu0gle5Dg","colab_type":"code","colab":{}},"source":["def plot_hist(arrs):\n","  plt.figure(figsize=(12,5))\n","  plt.hist(arrs,\n","    bins = 60,\n","    color = ['blue', '#D72F1A'],\n","    # edgecolor = 'black',\n","    label=[\"Same dists\", \"Diff dists\"],\n","    density=True\n","  )\n","  plt.legend(loc='upper right')\n","\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw6KfMEPU7-h","colab_type":"text"},"source":["## Display trained model stats"]},{"cell_type":"code","metadata":{"id":"l_TylF5AWw56","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TRAINING STATS =====\")\n","same_dists, diff_dists = eval_dists_on_sample(predict, train_datas_sample)\n","\n","print(\"Accuracy : {}%\".format(evaluate_accuracy_train_datas()*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","\n","plot_hist([same_dists, diff_dists])\n","print(len(train_images), ACCURACY_SAMPLE_SIZE, ACCURACY_SAMPLE_SIZE * len(train_images))\n","print(len(same_dists), len(diff_dists))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKOpRm8wdIdr","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TESTING STATS =====\")\n","same_dists, diff_dists = eval_dists_on_sample(predict_testing, test_datas_sample)\n","\n","print(\"Accuracy : {}%\".format(evaluate_accuracy_test_datas()*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","plot_hist([same_dists, diff_dists])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R_SwHLuYwA8A","colab":{}},"source":["# %matplotlib inline\n","\n","# for _ in range(3):\n","#     try:\n","#         pl.clf()\n","#         pl.plot(pd.Series(data=np.random.randn(100), index=i))\n","#         display.display(pl.gcf())\n","#         display.clear_output(wait=True)\n","#         time.sleep(1)\n","#     except KeyboardInterrupt:\n","#         break"],"execution_count":0,"outputs":[]}]}