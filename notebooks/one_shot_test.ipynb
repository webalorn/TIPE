{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"one_shot_test.ipynb","provenance":[{"file_id":"1Rbx4BF6wTy7awS4AOkslGo-52zgMc3X9","timestamp":1578148109303},{"file_id":"19Ugd6xcClNpUoRyxAwUMTNXzlR9T7Tq5","timestamp":1573574567851},{"file_id":"1lQ-PEDSlF-3xzhYXbTH-Ze1Q9hASbKP3","timestamp":1566407371074}],"collapsed_sections":["Dg26SMGNUthi","p5d2tWuMVHfA","LnO97_TKW26s","jgTfHHLOHrmC","wtCQ8tilIKkk","nvrjm90Ficov","RofKjrjsqobE"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m0w2omm1UJ9S","colab_type":"text"},"source":["# One-shot learning implementation for fish recognition\n","Using the [fish4knowledge Fish Recognition Ground-Truth dataset](https://github.com/brendenlake/omniglot/tree/master/python)"]},{"cell_type":"markdown","metadata":{"id":"FhnOva4D5GZQ","colab_type":"text"},"source":["### TODO\n","- entrainer sans negatif ?\n","- normalization des entrées (pour same ?)\n","- data augmentation ?\n","- comparer avec et sans\n","  - batch norm\n","  - batch norm au début\n","  - dropout\n","  - regul"]},{"cell_type":"code","metadata":{"id":"Gph_E9QuwQS4","colab_type":"code","outputId":"a609ca8c-5669-403c-939f-8f14a0e0948c","executionInfo":{"status":"ok","timestamp":1578215290796,"user_tz":-60,"elapsed":22452,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4mTAcOcdUd93","colab_type":"text"},"source":["## Setup phase\n","We install packages, make all imports, configure modules and download dataset"]},{"cell_type":"code","metadata":{"id":"I1246o-3RW3v","colab_type":"code","outputId":"d324b7b0-da70-47d7-8a6e-e805b646e474","executionInfo":{"status":"ok","timestamp":1578215399309,"user_tz":-60,"elapsed":130948,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":513}},"source":["%%bash\n","pip install -q pyyaml\n","pip install tensorflow==2.0.0-beta1\n","pip install -q tensorflow-gpu==2.0.0-beta1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0-beta1\n","  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n","  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n","Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n","  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.17.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.6)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (42.0.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n","Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DHAExJYpSHIb","colab_type":"code","outputId":"47802aa4-76db-4bdf-bc0b-c898a216c084","executionInfo":{"status":"ok","timestamp":1578215417093,"user_tz":-60,"elapsed":148723,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%%bash\n","git clone https://github.com/brendenlake/omniglot\n","mkdir datas\n","unzip -q omniglot/python/images_background.zip -d datas\n","unzip -q omniglot/python/images_evaluation.zip -d datas"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'omniglot'...\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dy0U2QJJRYmd","colab_type":"code","outputId":"89c01c5c-d30f-48ae-c958-dc5036fc2e6b","executionInfo":{"status":"ok","timestamp":1578215420844,"user_tz":-60,"elapsed":152461,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%load_ext tensorboard\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pl\n","import pandas as pd\n","import numpy as np\n","import skimage\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import datasets, layers, models\n","from tensorboard import notebook\n","from keras import backend as K\n","from IPython import display\n","\n","import os, datetime, time, math, pathlib, itertools, random\n","\n","keras = tf.keras\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","print(tf.version.VERSION)\n","print(tf.keras.__version__)\n","print(\"GPU Available: \", tf.test.is_gpu_available())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["2.0.0-beta1\n","2.2.4-tf\n","GPU Available:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_af6yejxduUq","colab_type":"text"},"source":["## Constants\n","This part will define how to build, train, and evaluate the model"]},{"cell_type":"code","metadata":{"id":"l16hgYf3UHel","colab_type":"code","cellView":"both","outputId":"16423df9-61df-44d2-c281-66524ba032bd","executionInfo":{"status":"ok","timestamp":1578215420845,"user_tz":-60,"elapsed":152432,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@markdown ## Data paths\n","DIR_DATAS = \"datas/images_background\" #@param {type:\"string\"}\n","DIR_DATA_TEST = \"datas/images_evaluation\" #@param {type:\"string\"}\n","LOAD_FROM = \"\"  #@param [\"\", \"save_dir\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_pretrained_1/weights_4_22_25.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_conv/weights.hdf5\", \"/content/drive/My Drive/ml/weights/oneshot_fishes/checkpoint_conv2/weights.hdf5\"] {allow-input: true}\n","CHECKPOINTS_DIR = \"drive/My Drive/ml/weights/oneshot_test\" #@param {type:\"string\"}\n","checkpoint_dir_name = \"checkpoint_\" + str(int(time.time()))\n","\n","#@markdown ## Model configuration\n","MODEL_TYPE = \"pretrained_2\" #@param [\"same\", \"density\", \"linear\",\"dense\", \"conv\", \"conv_2\", \"conv_3\", \"pretrained_1\", \"pretrained_2\", \"pretrained_3\", \"mobile_net\", \"mobile_net_feature\"]\n","IMG_SIDE = 100 #@param {type:\"slider\", min:10, max:300, step:1}\n","NB_CHANNEL = 0 #@param {type:\"number\"}\n","IMG_SHAPE = (IMG_SIDE, IMG_SIDE, NB_CHANNEL) if NB_CHANNEL else (IMG_SIDE, IMG_SIDE)\n","IMG_RESHAPE_CONV = (IMG_SIDE, IMG_SIDE, NB_CHANNEL or 1)\n","\n","#@markdown ## Training configuration\n","NB_EPOCHS = 100 #@param {type:\"number\"}\n","BATCH_SIZE = 32 #@param {type:\"number\"}\n","TRIPLETS_PER_IMAGE = 10 #@param {type:\"number\"}\n","LEARNING_RATE = 0.000000001 #@param {type:\"number\"}\n","MARGIN = 1 #@param {type:\"number\"}\n","\n","#@markdown ## Evaluation configuration\n","KEEP_TO_TRAIN = 10 #@param {type:\"number\"}\n","ACCURACY_SAMPLE_SIZE = 400 #@param {type:\"number\"}\n","\n","\n","checkpoint_dir_name = \"checkpoint_\" + MODEL_TYPE\n","print(\"Saving in {} for this session\".format(checkpoint_dir_name))\n","if LOAD_FROM:\n","  print(\"Loading weights from checkpoint {}\".format(LOAD_FROM))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Saving in checkpoint_pretrained_2 for this session\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dg26SMGNUthi","colab_type":"text"},"source":["## General code\n","Helper functions"]},{"cell_type":"code","metadata":{"id":"435-vCZcqPyc","colab_type":"code","colab":{}},"source":["def getRandomIds(dataset, nMax=1000):\n","\tids = list(range(len(dataset[0][0])-1))\n","\trandom.shuffle(ids)\n","\tids = ids[:nMax]\n","\treturn ids + [i+1 for i in ids]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG591ol1s-zz","colab_type":"code","colab":{}},"source":["  def dist_fct(x, y):\n","    return np.sqrt(np.sum((x-y)**2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7BKheGyefQY","colab_type":"code","colab":{}},"source":["def get_checkpoint_path(suffix=\"\"):\n","  os.makedirs(os.path.join(CHECKPOINTS_DIR, checkpoint_dir_name), exist_ok=True)\n","  return os.path.join(\n","    CHECKPOINTS_DIR,\n","    checkpoint_dir_name,\n","    \"weights\" + suffix + \".hdf5\"\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OBDI3NQtQ9u","colab_type":"code","colab":{}},"source":["class Timer():\n","  def __init__(self, to_int = True):\n","    self.t = time.time()\n","    self.to_int = to_int\n","  \n","  def get(self, reset=True):\n","    t2 = time.time()\n","    d = t2 - self.t\n","    if self.to_int:\n","      d = int(d)\n","    if reset:\n","      self.t = t2\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuTeqs4BqpS6","colab_type":"code","colab":{}},"source":["def plot_history(histories, key='binary_crossentropy'):\n","  plt.figure(figsize=(16,10))\n","\n","  for name, history in histories:\n","    val = plt.plot(history.epoch, history.history['val_'+key],\n","                   '--', label=name.title()+' Val')\n","    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n","             label=name.title()+' Train')\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(key.replace('_',' ').title())\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uejqNWClgtrE","colab_type":"code","colab":{}},"source":["def show_image(image):\n","\tplt.imshow(image)\n","\tplt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5d2tWuMVHfA","colab_type":"text"},"source":["## Import datas and pre-processing"]},{"cell_type":"code","metadata":{"id":"gLyuKF73v_Hh","colab_type":"code","colab":{}},"source":["def img_to_rgb(image):\n","  if len(image.shape) == 3 and image.shape[2] == 3:\n","    return image\n","  if len(image.shape) == 3:\n","    image = image.reshape(image.shape[:2])\n","  return skimage.color.grey2rgb(image)\n","\n","def reshape_image(image):\n","  h, w = image.shape[0], image.shape[1]\n","  scale = min(IMG_SHAPE[0]/h, IMG_SHAPE[1]/w)\n","  padH = round((IMG_SHAPE[0] / scale - h) / 2)\n","  padW = round((IMG_SHAPE[1] / scale - w) / 2)\n","\n","  padShape = ((padH, padH), (padW, padW), (0,0))\n","  image = skimage.util.pad(image, padShape, 'constant')\n","\n","  return skimage.transform.resize(image, IMG_SHAPE, mode='symmetric', preserve_range=True)\n","\n","def preprocess_image(tf_image):\n","  tf_image = tf.image.decode_image(tf_image)\n","  # tf_image = tf.image.resize(tf_image, IMG_SHAPE)\n","  image = tf_image.numpy().astype(float).reshape(tf_image.shape) / 255.0\n","  if NB_CHANNEL == 3:\n","    image = img_to_rgb(image)\n","  image = reshape_image(image)\n","  if NB_CHANNEL == 0:\n","    image = np.average(image, axis=2)\n","  return image\n","\n","def load_and_preprocess_image(img_path):\n","  return preprocess_image(tf.io.read_file(str(img_path)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5Ha_hh5q9Sq","colab_type":"code","colab":{}},"source":["def list_dir_files(path):\n","  return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","def get_sub_folder_cls(img_classes, img_dirs):\n","  cls = [\n","    [(os.path.join(cls_path, subfolder), subfolder[-2:] + \"_\" + label) for subfolder in os.listdir(cls_path)]\n","    for cls_path, label in zip(img_dirs, img_classes)\n","  ]\n","  cls = list(itertools.chain(*cls))\n","  return [c[1] for c in cls], [c[0] for c in cls]\n","\n","def get_datas_paths(dir_path):\n","  img_classes = sorted(list(os.listdir(dir_path)))\n","  img_dirs = [os.path.join(dir_path, cls) for cls in img_classes]\n","  img_classes, img_dirs = get_sub_folder_cls(img_classes, img_dirs)\n","  img_infos = [\n","      [(os.path.join(cls_path, img_f_name), label) for img_f_name in list_dir_files(cls_path)]\n","      for cls_path, label in zip(img_dirs, list(range(len(img_dirs))))\n","  ]\n","  img_infos = list(itertools.chain(*img_infos))\n","  paths, labels = [[el[i] for el in img_infos] for i in range(2)]\n","  return paths, labels, img_classes # len : nb images | nb images | nb classes\n","\n","def load_datas(dir_path):\n","  paths, labels, cls_names = get_datas_paths(dir_path)\n","  images_datas = [load_and_preprocess_image(img_path) for img_path in paths]\n","  images_datas, labels = np.array(images_datas), np.array(labels)\n","  return images_datas, labels, cls_names\n","\n","def get_subsets_per_cls(datas, labels, subsets=[None]):\n","  nb_cls = max(labels)+1\n","  subsets_datas = [[() for _ in subsets] for _ in range(nb_cls)]\n","  ids_per_cls = [[] for _ in range(nb_cls)]\n","  for i in range(len(datas)):\n","    ids_per_cls[labels[i]].append(i)\n","  \n","  for i_cls in range(nb_cls):\n","    for i_sub, max_datas in enumerate(subsets):\n","      if max_datas == None:\n","        max_datas = len(ids_per_cls[i_cls])\n","      subsets_datas[i_cls][i_sub] = ids_per_cls[i_cls][:max_datas]\n","      ids_per_cls[i_cls] = ids_per_cls[i_cls][max_datas:]\n","  split_datas = [([], []) for _ in subsets]\n","  for i_cls in range(nb_cls):\n","    for i_sub in range(len(subsets)):\n","      for i_img in subsets_datas[i_cls][i_sub]:\n","        split_datas[i_sub][0].append(datas[i_img])\n","        split_datas[i_sub][1].append(labels[i_img])\n","  split_datas = [(np.array(a1), np.array(a2)) for (a1, a2) in split_datas]\n","  return split_datas"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0jvq2J8UItF","colab_type":"text"},"source":["Now, we read all the datas"]},{"cell_type":"code","metadata":{"id":"EXXibU93QljQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"8e0f98c3-39b9-439e-f9f3-9258ec22a220","executionInfo":{"status":"ok","timestamp":1578215519725,"user_tz":-60,"elapsed":250811,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}}},"source":["try:\n","  _ = train_images\n","except:\n","  train_images, train_labels, train_cls_names = load_datas(DIR_DATAS)\n","  test_images, test_labels, test_cls_names = load_datas(DIR_DATA_TEST)\n","print(len(train_images), len(train_labels))\n","print(len(test_images), len(test_labels))\n","print(train_images[0].shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["19280 19280\n","13180 13180\n","(100, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LnO97_TKW26s","colab_type":"text"},"source":["## Functions to feed datas to the network"]},{"cell_type":"markdown","metadata":{"id":"7Z7tX_tCazT7","colab_type":"text"},"source":["First, helper functions to :\n","- get all images classed by label"]},{"cell_type":"code","metadata":{"id":"G4utCJ0DW6si","colab_type":"code","colab":{}},"source":["def get_ids_per_cls(labels):\n","  ids_per_cls = []\n","  for i in range(len(labels)):\n","    while len(ids_per_cls) <= labels[i]:\n","      ids_per_cls.append([])\n","    ids_per_cls[labels[i]].append(i)\n","  return ids_per_cls\n","\n","def sort_by_distance(l, anchor, only_ids=True):\n","  l2 = [(dist_fct(el, anchor), i) for i, el in enumerate(l)]\n","  l2.sort()\n","  if only_ids:\n","    return [i for d, i in l2]\n","  return [l[i] for d, i in l2]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USG4PW7hbDJ9","colab_type":"text"},"source":["The following function select random triplets to train the NN"]},{"cell_type":"code","metadata":{"id":"eWsnArbWa-px","colab_type":"code","colab":{}},"source":["def get_triplets_random(images, labels, trunk_model):\n","  print(\"get_triplets_random\", len(images), len(labels))\n","  nb_images = len(images)\n","  ids_per_cls = get_ids_per_cls(labels)\n","  triplets = []\n","  \n","  for i_anchor in range(nb_images):\n","    same_cls = [i for i in ids_per_cls[labels[i_anchor]] if i != i_anchor]\n","    if not same_cls:\n","      same_cls = [i_anchor]\n","    for _ in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = random.choice(same_cls), i_anchor\n","      while labels[i_negative] == labels[i_anchor]:\n","        i_negative = random.randint(0, nb_images-1)\n","      triplets.append([i_anchor, i_positive, i_negative])\n","  return triplets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qq90WWVLDrJ","colab_type":"text"},"source":["This functions try to select triplets better than random ones. At first, we run the NN on all examples. Then, we try to select triplets with a positive far from the anchor, and a negative close to it."]},{"cell_type":"code","metadata":{"id":"xtaHxTLzLCzT","colab_type":"code","colab":{}},"source":["# %%time\n","NB_CENTERS_PER_IMAGE = 5\n","\n","def get_triplets_dists(images, labels, trunk_model):\n","  timer = Timer()\n","  coords = trunk_model.predict(np.array(images))\n","  ids_per_cls = get_ids_per_cls(labels)\n","  centers = [np.mean([coords[i] for i in ids_per_cls[lab]], axis=0) for lab in range(len(ids_per_cls))]\n","\n","  centers_near_cls = [[] for _ in range(len(ids_per_cls))]\n","  for i_cls in range(len(ids_per_cls)):\n","    centers_sorted = sort_by_distance(centers, centers[i_cls])\n","    centers_sorted = [i_center for i_center in centers_sorted if i_center != i_cls][:NB_CENTERS_PER_IMAGE]\n","    centers_near_cls[i_cls] = centers_sorted\n","  \n","  triplets = []\n","  useful, unuseful = 0, 0\n","  for anchor in range(len(images)):\n","    same_cls = [i for i in ids_per_cls[labels[anchor]] if i != anchor] or [anchor]\n","    positives_order = sort_by_distance([coords[i] for i in same_cls], anchor)[::-1]\n","    positives = [same_cls[i] for i in positives_order]\n","\n","    centers_taken = centers_near_cls[labels[anchor]]\n","    negatives = list(itertools.chain(*[ids_per_cls[i_cls] for i_cls in centers_taken]))\n","    negatives_order = sort_by_distance([coords[i] for i in negatives], anchor)\n","    negatives = [negatives[i] for i in negatives_order]\n","\n","    for i in range(TRIPLETS_PER_IMAGE):\n","      i_positive, i_negative = i%len(positives), i%len(negatives)\n","      dist_diff = dist_fct(coords[anchor], coords[positives[i_positive]]) - dist_fct(coords[anchor], coords[negatives[i_negative]])\n","      if dist_diff + MARGIN < 0:\n","        unuseful += 1\n","      else:\n","        useful += 1\n","      triplets.append([anchor, positives[i_positive], negatives[i_negative]])\n","      # print(triplets[-1], [labels[j] for j in triplets[-1]])\n","  \n","  print(\"\\ntriplets computed\", timer.get(), \"s\", \"(useful, unuseful) =\", (useful, unuseful), \"({:.2f}%)\".format(useful / (useful + unuseful) * 100))\n","\n","  return triplets\n","\n","# triplets = get_triplets_dists(train_images, train_labels, trunk_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw5VslCJbIRI","colab_type":"text"},"source":["Now, we need a function to generate triplets during the training process. This function will be called by `fit_generator`"]},{"cell_type":"code","metadata":{"id":"zIHip7tWbCFy","colab_type":"code","colab":{}},"source":["last_triplets = []\n","def create_triplet_generator(images, labels, trunk_model, triplets_getter, batch_size):\n","  global last_triplets\n","  triplets = []\n","  cur_triplet = 0\n","  while True:\n","    if cur_triplet + batch_size > len(triplets):\n","      triplets = triplets_getter(images, labels, trunk_model)\n","      random.shuffle(triplets)\n","      cur_triplet = 0\n","    last_triplets = triplets\n","\n","    yield (\n","      [ np.array([images[triplets[cur_triplet + i_triplet][i_in]] for i_triplet in range(batch_size)]) for i_in in range(3)],\n","      [0] * batch_size\n","    )\n","\n","    cur_triplet += batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vx1Ob-kU4JD","colab_type":"text"},"source":["## Model definition"]},{"cell_type":"markdown","metadata":{"id":"ZXnRDBC0HwN0","colab_type":"text"},"source":["### Simple models"]},{"cell_type":"code","metadata":{"id":"ZJpPZYsIH2i2","colab_type":"code","colab":{}},"source":["def create_same_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Flatten(),\n","  ], name=\"same_model\")\n","  return model\n","\n","def create_density_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Lambda(lambda x : tf.math.reduce_mean(x, axis=(1,2)))\n","    # layers.Flatten(),\n","  ], name=\"same_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dX6A30xTUkrf","colab_type":"code","colab":{}},"source":["def create_dense_trunk_model():\n","  model = keras.models.Sequential([\n","    get_mobile_net(train_layers=11), # 11\n","    # layers.GlobalAveragePooling2D(),\n","    layers.Flatten(),\n","    layers.Dense(2000, activation='relu'),\n","    layers.Dense(400, activation='relu'), # 64 ?\n","    layers.Dense(64, activation='relu'), # 64 ?\n","  ], name=\"dense_model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlQsyRUdH9-A","colab_type":"code","colab":{}},"source":["def create_conv_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Reshape(IMG_RESHAPE_CONV),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(20, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(40, (5, 5), activation='relu'),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(300, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(100, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model\")\n","  return model\n","\n","def create_conv_2_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Reshape(IMG_RESHAPE_CONV),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(64, (8, 8), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(128, (8, 8), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","\n","    layers.Conv2D(256, (4, 4), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(512, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l=1e-4)),\n","    # layers.Dense(2048, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l=1e-4)),\n","    # layers.Dense(32 , activation='tanh'),\n","\n","    # layers.Flatten(),\n","    # layers.Dense(300, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(100, activation='relu'),\n","    # # layers.Dropout(0.2),\n","    # layers.Dense(32, activation='softmax')\n","  ], name=\"conv_model_2\")\n","  return model\n","\n","def create_conv_3_trunk_model():\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),\n","    layers.Reshape(IMG_RESHAPE_CONV),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(32, (8, 8), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)), # kernel_regularizer=keras.regularizers.l2(1e-4)\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(64, (8, 8), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Conv2D(128, (4, 4), activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n","    layers.MaxPool2D((2, 2)),\n","    layers.BatchNormalization(),\n","\n","    layers.Flatten(),\n","    layers.Dense(64, activation='tanh'),\n","  ], name=\"conv_model_3\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgTfHHLOHrmC","colab_type":"text"},"source":["### With mobile net"]},{"cell_type":"code","metadata":{"id":"16g9L4aUwCEK","colab_type":"code","colab":{}},"source":["def get_mobile_net(train_layers=0):\n","  model = tf.keras.applications.MobileNetV2(\n","    input_shape=IMG_SHAPE,\n","    include_top=False,\n","    weights='imagenet'\n","  )\n","  model.trainable = False\n","  if train_layers:\n","    model.trainable = True\n","    for layer in model.layers[:-train_layers]:\n","      layer.trainable =  False\n","  return model\n","\n","def create_mobile_net_trunk_model():\n","  model = keras.models.Sequential([\n","    get_mobile_net(train_layers=0), # 11\n","    layers.Conv2D(32, (4, 4), activation='relu'),\n","    layers.GlobalAveragePooling2D(),\n","    # layers.Flatten(),\n","    # layers.Dense(32, activation='relu'),\n","  ], name=\"linear_model\")\n","  return model\n","\n","def create_mobile_net_feature_trunk_model():\n","  model = keras.models.Sequential([\n","    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_075_96/feature_vector/4\",\n","                    trainable=False),\n","    layers.Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l=1e-4)),\n","  ], name=\"linear_model\")\n","  model.build([None, 96, 96, 3])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtCQ8tilIKkk","colab_type":"text"},"source":["### With pre-trained model"]},{"cell_type":"code","metadata":{"id":"7uWBDd1cIxhA","colab_type":"code","colab":{}},"source":["def get_pretrained_back(remove=1, trainable=2):\n","  checkpoint_path = \"drive/My Drive/ml/weights/omniglot_conv/checkpoint_conv_3/weights_1.1.hdf5\"\n","  # model = keras.models.load_model(checkpoint_path, compile=False)\n","\n","  model = keras.models.Sequential([\n","    layers.Input(IMG_SHAPE),layers.BatchNormalization(),layers.Reshape(IMG_SHAPE+(1,)),\n","    layers.Conv2D(32, (8, 8), activation='relu', padding=\"same\"),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(64, (4, 4), activation='relu', padding=\"same\"),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(128, (4, 4), activation='relu', padding=\"same\"),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Conv2D(256, (4, 4), activation='relu', padding=\"same\"),layers.MaxPool2D((2, 2)),layers.BatchNormalization(),\n","    layers.Flatten(),layers.Dense(4096, activation='tanh'),layers.Dense(1623, activation='softmax')\n","  ], name=\"conv_3_model\")\n","\n","  model.load_weights(checkpoint_path)\n","\n","  for _ in range(remove):\n","    model.pop()\n","  \n","  for l in model.layers[:len(model.layers)-trainable]:\n","    l.trainable = False\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GKR-VSFXJFpk","colab_type":"code","colab":{}},"source":["def create_pretrained_1_trunk_model():\n","  model = get_pretrained_back(remove=3, trainable=3)\n","  model.add(layers.Dense(512, activation=\"tanh\"))\n","\n","  return model\n","\n","def create_pretrained_2_trunk_model():\n","  model = get_pretrained_back(remove=3, trainable=3)\n","  # model.add(layers.Conv2D(512, (4, 4), activation='relu', padding=\"same\"))\n","  model.add(layers.GlobalAveragePooling2D())\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02rcAoUjUziy","colab_type":"text"},"source":["### Create the siamese network"]},{"cell_type":"markdown","metadata":{"id":"RlCDXXUVU_Mp","colab_type":"text"},"source":["The first function, given a model, create a siamese NN with this model as the common part. The second create a model with two times the same siamese network to compute triplet loss."]},{"cell_type":"code","metadata":{"id":"OoW1fHWqU-SL","colab_type":"code","colab":{}},"source":["def create_siamese(trunk_model):\n","  inputs = [layers.Input(IMG_SHAPE) for _ in range(2)]\n","  parts = [trunk_model(inTensor) for inTensor in inputs]\n","  diff = layers.subtract(parts)\n","  out = layers.Lambda(lambda x : tf.reduce_sum(x**2, axis=(1,)))(diff)\n","  out_sqrt = layers.Lambda(lambda x : tf.sqrt(x))(out)\n","  return keras.models.Model(inputs=inputs, outputs=out_sqrt, name=\"Siamese_model\"+\"_\"+trunk_model.name)\n","\n","def create_triplet_siamese(siamese_model, margin=1.0):\n","  in_anchor, in_positive, in_negative = [layers.Input(IMG_SHAPE, name=name) for name in [\"in_anchor\", \"in_positive\", \"in_negative\"]]\n","  positive_dist = siamese_model([in_anchor, in_positive])\n","  negative_dist = siamese_model([in_anchor, in_negative])\n","\n","  dist = layers.subtract([positive_dist, negative_dist])\n","  if margin:\n","    dist = layers.Lambda(lambda x : tf.maximum(x + margin, 0.0))(dist)\n","  # dist = layers.Lambda(lambda x : tf.square(x))(dist) # keep ?\n","  return keras.models.Model(inputs=[in_anchor, in_positive, in_negative], outputs=dist, name=\"Siamese_triplet_model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO2W3mjSU_t3","colab_type":"text"},"source":["We will now choose the model we will use"]},{"cell_type":"code","metadata":{"id":"1sp3PzQwUvMw","colab_type":"code","outputId":"494b9b74-bffd-4f63-c9f3-0069983289ee","executionInfo":{"status":"ok","timestamp":1578215525535,"user_tz":-60,"elapsed":256145,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["trunk_model = globals()[\"create_{}_trunk_model\".format(MODEL_TYPE)]()\n","siamese_model = create_siamese(trunk_model)\n","model = create_triplet_siamese(siamese_model, margin=MARGIN)\n","trunk_model.summary()\n","model.summary()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Model: \"conv_3_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","batch_normalization (BatchNo (None, 100, 100)          400       \n","_________________________________________________________________\n","reshape (Reshape)            (None, 100, 100, 1)       0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 100, 100, 32)      2080      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 50, 50, 32)        0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 50, 50, 32)        128       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 50, 50, 64)        32832     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 25, 25, 64)        0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 25, 25, 64)        256       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 25, 25, 128)       131200    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 12, 12, 256)       524544    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 256)               0         \n","=================================================================\n","Total params: 692,976\n","Trainable params: 525,056\n","Non-trainable params: 167,920\n","_________________________________________________________________\n","Model: \"Siamese_triplet_model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","in_anchor (InputLayer)          [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","in_positive (InputLayer)        [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","in_negative (InputLayer)        [(None, 100, 100)]   0                                            \n","__________________________________________________________________________________________________\n","Siamese_model_conv_3_model (Mod (None,)              692976      in_anchor[0][0]                  \n","                                                                 in_positive[0][0]                \n","                                                                 in_anchor[0][0]                  \n","                                                                 in_negative[0][0]                \n","__________________________________________________________________________________________________\n","subtract_1 (Subtract)           (None,)              0           Siamese_model_conv_3_model[1][0] \n","                                                                 Siamese_model_conv_3_model[2][0] \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None,)              0           subtract_1[0][0]                 \n","==================================================================================================\n","Total params: 692,976\n","Trainable params: 525,056\n","Non-trainable params: 167,920\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pjsdV_yIc2Hr","colab_type":"text"},"source":["If there's a model to restore, we will try to restore weights"]},{"cell_type":"code","metadata":{"id":"zOzNhmkjc1Pw","colab_type":"code","outputId":"684d0f52-cb5e-48ea-8370-ed5eaaaf61ea","executionInfo":{"status":"ok","timestamp":1578215525536,"user_tz":-60,"elapsed":256120,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if LOAD_FROM:\n","  if LOAD_FROM == \"save_dir\":\n","    LOAD_FROM = get_checkpoint_path()\n","  print(\"Load weights from\", LOAD_FROM)\n","  # model.load_weights(LOAD_FROM)\n","  # model = tf.keras.models.load_model(LOAD_FROM)\n","  trunk_model.load_weights(LOAD_FROM)\n","else:\n","  print(\"No weights to load\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["No weights to load\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nvrjm90Ficov","colab_type":"text"},"source":["## Prediction and evaluation models and functions"]},{"cell_type":"code","metadata":{"id":"MsjhioL9ij_Y","colab_type":"code","colab":{}},"source":["class NearestPredictor:\n","  def __init__(self, trunk_model, datas=([], [])):\n","    self.set_datas(datas)\n","    self.trunk_model = trunk_model\n","  \n","  def set_datas(self, datas):\n","    self.images, self.labels = datas\n","  \n","  def build(self):\n","    self.img_coords = self.trunk_model.predict(np.array(self.images))\n","\n","  def predict_in_datas(self, i):\n","    dists = [dist_fct(self.img_coords[i], coord) for coord in self.img_coords]\n","    dists[i] = max(dists) * 2 + 1\n","    return self.labels[np.argmin(dists)]\n","\n","  def predict(self, image):\n","    predict_coords = self.trunk_model.predict(np.array([image]))[0]\n","    dists = [dist_fct(predict_coords, coord) for coord in self.img_coords]\n","    return self.labels[np.argmin(dists)]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW1Z7O66h5JO","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_in_datas(predict_obj, ids_sample):\n","  good_results = [i for i in ids_sample if predict_obj.labels[i] == predict_obj.predict_in_datas(i)]\n","  return (len(good_results) / len(ids_sample), good_results)\n","\n","def evaluate_accuracy(predict_obj, datas, labels, ids_sample=None):\n","  if ids_sample == None:\n","    ids_sample = list(range(len(datas)))\n","  good_results = [i for i in ids_sample if labels[i] == predict_obj.predict(datas[i])]\n","  return (len(good_results) / len(ids_sample), good_results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D64gMg6W1wW1","colab_type":"code","colab":{}},"source":["def evaluate_accuracy_train_datas():\n","  return evaluate_accuracy_in_datas(predict, train_datas_sample)\n","\n","def evaluate_accuracy_test_datas():\n","  return evaluate_accuracy_in_datas(predict_testing, test_datas_sample)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkwMelCGgdGa","colab_type":"code","colab":{}},"source":["def random_sample_homogeneous(labels, sample_size):\n","  nb_cls = max(labels)+1\n","  ids_of_cls = [[] for _ in range(nb_cls)]\n","  for i, cls in enumerate(labels):\n","    ids_of_cls[cls].append(i)\n","  \n","  per_cls = sample_size // nb_cls\n","  taken, not_taken = [], []\n","  for ids in ids_of_cls:\n","    random.shuffle(ids)\n","    taken.extend(ids[:per_cls])\n","    not_taken.extend(ids[per_cls:])\n","  if len(taken) < sample_size:\n","    taken.extend(random.sample(not_taken, sample_size-len(taken)))\n","  return taken\n","\n","predict = NearestPredictor(trunk_model, (train_images, train_labels))\n","predict_testing = NearestPredictor(trunk_model, (test_images, test_labels))\n","# predict_base = NearestPredictor(trunk_model, (test_base_images, test_base_labels))\n","\n","train_datas_sample = random.sample(list(range(len(train_labels))), ACCURACY_SAMPLE_SIZE)\n","test_datas_sample = random.sample(list(range(len(test_labels))), ACCURACY_SAMPLE_SIZE)\n","# train_datas_sample = random_sample_homogeneous(train_labels, len(train_labels))\n","# test_datas_sample = random_sample_homogeneous(test_labels, len(test_labels))\n","# print(len(train_labels), len(test_labels))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RofKjrjsqobE","colab_type":"text"},"source":["## Monitor and prepare training"]},{"cell_type":"code","metadata":{"id":"0xLiPakxzG6k","colab_type":"code","colab":{}},"source":["def triplet_loss(y_true, y_pred):\n","  return K.mean(y_pred)\n","\n","def accuracy_fct(y_true, y_pred):\n","  return K.mean(y_pred[:] <= 0.0000001)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEGo4MPargI2","colab_type":"text"},"source":["Training callbacks"]},{"cell_type":"code","metadata":{"id":"8ze4leqBq6s9","colab_type":"code","colab":{}},"source":["class EpochStdoutLoggerCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_begin(self, epoch, logs):\n","    self.time = time.time()\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    epoch_time = time.time() - self.time\n","    print(\"Epoch {}/{} finished in {}m {}s | loss: {:.5f} - accuracy: {:.5f}\".format(\n","      epoch+1, NB_EPOCHS, int(epoch_time) // 60, int(epoch_time) % 60, logs['loss'], logs['accuracy']\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPc_qdnGq-t3","colab_type":"code","colab":{}},"source":["class AccuracyCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, epoch_interval, accuracies=None):\n","    super().__init__()\n","    self.epoch_interval = epoch_interval\n","    self.accuracies = accuracies\n","  \n","  def on_epoch_end(self, epoch, logs):\n","    if epoch % self.epoch_interval == self.epoch_interval - 1:\n","      predict.build()\n","      acc, good_datas = evaluate_accuracy_in_datas(predict, train_datas_sample)\n","      print(\"\\nAccuracy : {}%\".format(acc * 100))\n","      if self.accuracies:\n","        self.accuracies.append(acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBJh91_7Egrc","colab_type":"code","colab":{}},"source":["class SaveTrunkCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, trunk, path, load_weights_on_restart=False, batch_interval=None):\n","    self.trunk = trunk\n","    self.path = path\n","    self.load_weights_on_restart = load_weights_on_restart\n","    self.batch_interval = batch_interval\n","  \n","  def save(self):\n","    self.trunk.save_weights(self.path)\n","\n","  def on_train_begin(self, logs=None):\n","    if (self.load_weights_on_restart and os.path.exists(self.path)):\n","      self.trunk.load_weights(self.path)\n","  \n","  def on_train_batch_end(self, batch, logs=None):\n","    if self.batch_interval and batch % self.batch_interval == 0:\n","      self.save()\n","  \n","  def on_epoch_end(self, epoch, logs=None):\n","    self.save()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Hp92KN7U6Mv","colab_type":"text"},"source":["## Training phase"]},{"cell_type":"code","metadata":{"id":"34mW85aOyD4B","colab_type":"code","colab":{}},"source":["# !rm -R logs/*\n","# %tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNxs015TxoRJ","colab_type":"code","colab":{}},"source":["model.compile(\n","    optimizer=tf.optimizers.Adam(learning_rate=0.00000001),\n","    # optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.00000001),\n","    # optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.0000001),\n","    loss=triplet_loss,\n","    metrics=[accuracy_fct]\n",")\n","accuracies=[]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGNJ0OZextKo","colab_type":"code","outputId":"101f7af2-5366-47bc-9df6-a19fe27c641c","executionInfo":{"status":"error","timestamp":1578216073695,"user_tz":-60,"elapsed":803968,"user":{"displayName":"Théophane Vallaeys","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNIiEdxRdrXMeXk0L2WJbHwF9gXI_FdGFTQq0I73U=s64","userId":"08573448592070946904"}},"colab":{"base_uri":"https://localhost:8080/","height":752}},"source":["triplet_generator = create_triplet_generator(\n","    train_images, train_labels, trunk_model,\n","    # get_triplets_dists,\n","    get_triplets_random,\n","    BATCH_SIZE\n",")\n","\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","checkpoint_path = get_checkpoint_path(suffix=\"_1\")\n","print(\"Saving weights at\", checkpoint_path)\n","\n","callbacks = [\n","  SaveTrunkCallback(trunk_model, checkpoint_path, load_weights_on_restart=True, batch_interval=1000),\n","  # EpochStdoutLoggerCallback(),\n","  AccuracyCallback(1, accuracies),\n","  # keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n","]\n","\n","r = model.fit_generator(\n","  generator=triplet_generator,\n","  epochs = NB_EPOCHS,\n","  steps_per_epoch = int(len(train_images) * TRIPLETS_PER_IMAGE / BATCH_SIZE),\n","  callbacks=callbacks,\n","  # verbose=0,\n","  # validation_data=(x_test, y_test)\n",")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Saving weights at drive/My Drive/ml/weights/oneshot_test/checkpoint_pretrained_2/weights_1.hdf5\n","get_triplets_random 19280 19280\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","6013/6025 [============================>.] - ETA: 0s - loss: 0.2169 - accuracy_fct: 0.6698get_triplets_random 19280 19280\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy_fct: 0.6698\n","Accuracy : 36.5%\n","6025/6025 [==============================] - 265s 44ms/step - loss: 0.2169 - accuracy_fct: 0.6698\n","Epoch 2/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237463). Check your callbacks.\n","   1/6025 [..............................] - ETA: 26:51 - loss: 0.2001 - accuracy_fct: 0.7188WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.118759). Check your callbacks.\n","6013/6025 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy_fct: 0.6753get_triplets_random 19280 19280\n","6024/6025 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy_fct: 0.6753\n","Accuracy : 36.25%\n","6025/6025 [==============================] - 256s 42ms/step - loss: 0.2128 - accuracy_fct: 0.6753\n","Epoch 3/100\n","WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.217721). Check your callbacks.\n","   1/6025 [..............................] - ETA: 24:38 - loss: 0.1692 - accuracy_fct: 0.7188WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.108879). Check your callbacks.\n"," 953/6025 [===>..........................] - ETA: 2:14 - loss: 0.2109 - accuracy_fct: 0.6779"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-a3a88597d589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mTRIPLETS_PER_IMAGE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;31m# verbose=0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# validation_data=(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3502\u001b[0m               'You must feed a value for placeholder %s' % (tensor,))\n\u001b[1;32m   3503\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3504\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3505\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m         \u001b[0;31m# Temporary workaround due to `convert_to_tensor` not casting floats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1098\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1099\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1100\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"8OM-CAjmeCSV","colab_type":"text"},"source":["## Functions to compute / plot stats about trained models"]},{"cell_type":"code","metadata":{"id":"3HYfUdZVeBs8","colab_type":"code","colab":{}},"source":["def eval_dists_on_sample(predict_obj, ids_sample):\n","  same_dists, diff_dists = [], []\n","\n","  for i in ids_sample:\n","    i_cls, i_coord = predict_obj.labels[i], predict_obj.img_coords[i]\n","    for j, j_coord in enumerate(predict_obj.img_coords):\n","      if i != j:\n","        if i_cls == predict_obj.labels[j]:\n","          same_dists.append(dist_fct(i_coord, j_coord))\n","        else:\n","          diff_dists.append(dist_fct(i_coord, j_coord))\n","  return same_dists, diff_dists"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brEHu0gle5Dg","colab_type":"code","colab":{}},"source":["def plot_hist(arrs):\n","  plt.figure(figsize=(12,5))\n","  plt.hist(arrs,\n","    bins = 60,\n","    color = ['blue', '#D72F1A'],\n","    # edgecolor = 'black',\n","    label=[\"Same dists\", \"Diff dists\"],\n","    density=True\n","  )\n","  plt.legend(loc='upper right')\n","\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-m-84ILPFYx","colab_type":"code","colab":{}},"source":["def plot_results_per_class(good_results, labels, cls_names, sample=None):\n","  n_cls = len(cls_names)\n","  cls_success, cls_failed = [0]*n_cls, [0]*n_cls\n","\n","  if n_cls > 60:\n","    plt.figure(figsize=(35,6))\n","  else:\n","    plt.figure(figsize=(15,6))\n","\n","  is_success = [False] * len(labels)\n","  if sample:\n","    is_success = [None] * len(labels) # None stand for unused\n","    for i in sample:\n","      is_success[i] = False\n","  \n","  for i in good_results:\n","    is_success[i] = True\n","\n","  for i, succ in enumerate(is_success):\n","    if succ == True:\n","      cls_success[labels[i]] += 1\n","    elif succ == False:\n","      cls_failed[labels[i]] += 1\n","  \n","  ind = np.arange(n_cls)\n","  width = 0.8 # the width of the bars: can also be len(x) sequence\n","  rotation = 45 if n_cls < 60 else 90\n","\n","  p1 = plt.bar(ind, cls_success, width, color=\"#4CAF50\")\n","  p2 = plt.bar(ind, cls_failed, width, bottom=cls_success, color=\"#EF5350\")\n","\n","  plt.ylabel('Number of tests')\n","  plt.xlabel('Fish species')\n","  plt.title('Number of detection success and failure per fish species')\n","  plt.xticks(ind, cls_names, rotation=rotation)\n","  # plt.yticks(np.arange(0, 81, 10))\n","  plt.legend((p1[0], p2[0]), ('Success', 'Failed'))\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw6KfMEPU7-h","colab_type":"text"},"source":["## Display trained model stats"]},{"cell_type":"code","metadata":{"id":"l_TylF5AWw56","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TRAINING STATS =====\")\n","predict.build()\n","print(\"Build 1\")\n","same_dists, diff_dists = eval_dists_on_sample(predict, train_datas_sample)\n","print(\"Build 2\")\n","accuracy, good_results = evaluate_accuracy_train_datas()\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","\n","plot_results_per_class(good_results, train_labels, train_cls_names, sample=train_datas_sample)\n","plot_hist([same_dists, diff_dists])\n","print(len(train_images), ACCURACY_SAMPLE_SIZE, ACCURACY_SAMPLE_SIZE * len(train_images))\n","print(len(same_dists), len(diff_dists))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKOpRm8wdIdr","colab_type":"code","colab":{}},"source":["%%time\n","print(\"===== TESTING WITHIN SAME DATAS STATS =====\")\n","predict_testing.build()\n","same_dists, diff_dists = eval_dists_on_sample(predict_testing, test_datas_sample)\n","accuracy, good_results = evaluate_accuracy_test_datas()\n","\n","print(\"Accuracy : {}%\".format(accuracy*100))\n","print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","\n","plot_results_per_class(good_results, test_labels, test_cls_names, sample=test_datas_sample)\n","plot_hist([same_dists, diff_dists])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFUfMeIYMKdE","colab_type":"code","colab":{}},"source":["# %%time\n","# print(\"===== TESTING WITH DIFFERENT DATAS STATS =====\")\n","# predict_base.build()\n","# # same_dists, diff_dists = eval_dists_on_sample(predict_base, test_datas_sample)\n","\n","# accuracy, good_results = evaluate_accuracy(predict_base, test_images, test_labels)\n","# print(\"Accuracy : {}%\".format(accuracy*100))\n","# plot_results_per_class(good_results, test_labels, test_cls_names)\n","\n","# # print(\"Avg dist same class :\", sum(same_dists) / len(same_dists))\n","# # print(\"Avg dist distinct classes :\", sum(diff_dists) / len(diff_dists))\n","# # plot_hist([same_dists, diff_dists])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R_SwHLuYwA8A","colab":{}},"source":["# %matplotlib inline\n","\n","# for _ in range(3):\n","#     try:\n","#         pl.clf()\n","#         pl.plot(pd.Series(data=np.random.randn(100), index=i))\n","#         display.display(pl.gcf())\n","#         display.clear_output(wait=True)\n","#         time.sleep(1)\n","#     except KeyboardInterrupt:\n","#         break"],"execution_count":0,"outputs":[]}]}